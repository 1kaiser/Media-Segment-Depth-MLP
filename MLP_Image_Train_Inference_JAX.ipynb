{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPluorvhF6eWN8nKDXHTDf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/Media-Segment-Depth-MLP/blob/main/MLP_Image_Train_Inference_JAX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "miM9-4pMQ23h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Downloading the dataset**"
      ],
      "metadata": {
        "id": "99FCZgbz2SkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "download [flower dataset](https://www.kaggle.com/datasets/alxmamaev/flowers-recognition?resource=download) from kaggle."
      ],
      "metadata": {
        "id": "go6vwNDjvoSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "!wget https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/archive.zip -O archive.zip\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive..\n",
        "output.clear()\n",
        "##########################<< copying all varities into a single folder block >>################\n",
        "!mkdir -p /content/flowers/all\n",
        "!cp /content/flowers/daisy/* /content/flowers/all\n",
        "!cp /content/flowers/dandelion/* /content/flowers/all\n",
        "!cp /content/flowers/rose/* /content/flowers/all\n",
        "!cp /content/flowers/sunflower/* /content/flowers/all\n",
        "!cp /content/flowers/tulip/* /content/flowers/all\n",
        "##########################<< end of block >>################\n",
        "print(\"creating single image folder complete >>>\")\n"
      ],
      "metadata": {
        "id": "ZrgH9Bng9x_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN** "
      ],
      "metadata": {
        "id": "bwhvgI06xTIu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI1aeBA-dCqC"
      },
      "source": [
        "**Model and training code**\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODING BLOCK** "
      ],
      "metadata": {
        "id": "cwyDAsz74bk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "z9aPWpu5iJ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP MODEL DEFINATION**\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "DllYcUtgovO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -qq -U flax orbax\n",
        "# Orbax needs to enable asyncio in a Colab environment.\n",
        "!python -m pip install -qq nest_asyncio\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        for i in range(ndl):\n",
        "            x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "VRkotxnvvHrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initialize the module**"
      ],
      "metadata": {
        "id": "4o02pjAJqdjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def Create_train_state(r_key, model, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    variables = model.init(r_key, jnp.ones(shape)) \n",
        "    optimizer = optax.adam(learning_rate) \n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "rJEuhCl5xuR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining loss function**"
      ],
      "metadata": {
        "id": "ixrJnhFt4B2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#serial\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2) \n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  metrics = {\n",
        "      'loss': loss,     #LOSS\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "aAz8hjEbu2Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train step defination**"
      ],
      "metadata": {
        "id": "GAlchTpb3fVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cpu serial\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    image, label = batch  \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);\n",
        "        loss =  image_difference_loss(logits, label);\n",
        "        return loss, logits\n",
        "\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = gradient_fn(state.params)\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    logs = compute_metrics(logits=logits, labels=label)\n",
        "    return new_state, logs\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, image):\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=image)\n"
      ],
      "metadata": {
        "id": "FJZR-rndueX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image conversion fiunctions**"
      ],
      "metadata": {
        "id": "d-1nI5W-LN7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu"
      ],
      "metadata": {
        "id": "OF3ajPCGLSU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image dataset, image size and batch size Setup**"
      ],
      "metadata": {
        "id": "Ns6S0Bmrxhyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## to load the data in batches as mentioned single batch of images with already provided sizes \n",
        "import jax\n",
        "from jax import random\n",
        "\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "batch_size = 1\n",
        "\n",
        "import os\n",
        "image_dir = r'/content/flowers/tulip/'\n",
        "#############################################################################\n",
        "bandend = [\"c\",\".jpg\", \"b02\"]\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "\n",
        "######################################## making 8 array of input for each device >>>\n",
        "def batchedimages(image_locations):\n",
        "  RGB8 = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1]))\n",
        "  GRAY8 = jnp.asarray((imageGRAY(total_images_path[image_locations[0]])[1]))\n",
        "  batch_ccc = RGB8, GRAY8 \n",
        "  return batch_ccc\n",
        "\n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  "
      ],
      "metadata": {
        "id": "B7IgRCG91RcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **ðŸ‘ HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from google.colab import output\n",
        "import orbax.checkpoint as orbax\n",
        "from flax.training import checkpoints\n",
        "\n",
        "import optax\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "CKPT_DIR = 'ckpts'\n",
        "\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  HxW, Channels = next(batches)[0].shape\n",
        "  state = Create_train_state( rng, model, (HxW, Channels), learning_rate ) \n",
        "  count = 1\n",
        "#âœ…âœ…ðŸ”» state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "###################### \n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "\n",
        "total_epochs = 50\n",
        "for epochs in tqdm(range(total_epochs)):  \n",
        "  batches = data_stream() \n",
        "\n",
        "  if checkpoint_available:\n",
        "    state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r {dir}\n",
        "\n",
        "  for bbb in tqdm(range(no_of_batches-5)):\n",
        "    state, metrics = train_step(state, next(batches), rng)\n",
        "    output.clear()\n",
        "    print(\"loss: \",metrics['loss'],\" <<< \") # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step=epochs, prefix='checkpoint_', keep=1, overwrite=False, orbax_checkpointer=orbax_checkpointer)\n",
        "  # restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "TH4A--qI31lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inference engine**"
      ],
      "metadata": {
        "id": "YchphIIkJIf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np \n",
        "from google.colab import output\n",
        "from flax.training import checkpoints\n",
        "\n",
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "image_in = '/content/a.jpg'\n",
        "\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "image = jnp.asarray((imageRGB(image_in)[1]))\n",
        "#restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "#state = restored_state\n",
        "#initialize\n",
        "HxW, Channels = image.shape\n",
        "state = Create_train_state( model, rng, (HxW, Channels), learning_rate ) \n",
        "\n",
        "state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "prediction = eval_step(state, image)\n",
        "prediction['loss']\n",
        "\n",
        "\n",
        "predicted_image = np.array(prediction['logits'],  dtype=np.uint8).reshape(newsize) \n",
        "cv2_imshow(predicted_image)\n"
      ],
      "metadata": {
        "id": "VmZn4n_-JL-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import numpy as np \n",
        "# def show_image(argu):\n",
        "#   L1 = argu[0]\n",
        "#   predicted_image = np.array(argu[0],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "#   a = predicted_image\n",
        "#   for i in range(0,argu.shape[0]):\n",
        "#     predicted_image = np.array(argu[i],  dtype=np.uint8).reshape(newsize) \n",
        "#     a = cv2.hconcat([a, predicted_image])\n",
        "#   cv2_imshow(a)\n",
        "\n",
        "# show_image(metrics['logits'])"
      ],
      "metadata": {
        "id": "teuUnNYweNNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from jax.tree_util import tree_structure\n",
        "# print(tree_structure(state))"
      ],
      "metadata": {
        "id": "LOK4VpYK53ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**test dataset segmentation Creation download section**"
      ],
      "metadata": {
        "id": "79e1MnTEOiYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##################################<<< MEDIAPIPE LIBRARY INSTALLATON >>>#############################\n",
        "!python -m pip install mediapipe\n",
        "##################################<<< FRAME EXTRACTION >>>#############################\n",
        "video_location = '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/test.mp4'\n",
        "import os\n",
        "\n",
        " \n",
        "# Read images with OpenCV.\n",
        "#images= None\n",
        "image_dir = '/content/MEDIAPIPEinput/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "image_dir_out = '/content/annotated_images'\n",
        "os.makedirs(image_dir_out, exist_ok=True)\n",
        "frame_rate = 25\n",
        "!ffmpeg -y -hwaccel cuvid \\\n",
        "  -i {video_location} \\\n",
        "  -r {frame_rate} {image_dir}out_%09d.png\n",
        "\n",
        "imgs_list = os.listdir(image_dir)\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n",
        "################################<<< SEGMENTATION USING MEDIAPIPE >>>###################################\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "# mp_holistic = mp.solutions.holistic\n",
        "mp_pose = mp.solutions.pose\n",
        "!rm -r {image_dir}.ipynb_checkpoints\n",
        "\n",
        "# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.\n",
        "with mp_pose.Pose(static_image_mode=True, \n",
        "                          min_detection_confidence=0.2,\n",
        "                          model_complexity=2, \n",
        "                          enable_segmentation=True,) as pose:\n",
        "  temp_segmentation_mask =[]                        \n",
        "  for name, image in enumerate(imgs_path):\n",
        "    !rm -r {image_dir}.ipynb_checkpoints\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "    image = cv2.imread(image)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # Draw pose segmentation.\n",
        "    print(f'Pose segmentation of {name}:')\n",
        "    annotated_image_pose = image.copy()\n",
        "    red_img = np.zeros_like(annotated_image_pose, dtype=np.uint8)\n",
        "    red_img[:, :] = (255,255,255)\n",
        "    ###check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    if results.segmentation_mask is None:\n",
        "      print(\"true\")\n",
        "      results.segmentation_mask = temp_segmentation_mask[-1]\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    else:\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    ###End check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    segm_2class = 0.0 + 1.0 * results.segmentation_mask\n",
        "    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "    annotated_image_pose = annotated_image_pose * segm_2class + red_img * (1 - segm_2class)\n",
        "    #resize_and_show(annotated_image)\n",
        "    cv2.imwrite('%s/%s' %(image_dir_out, imgs_list[name]), annotated_image_pose)\n",
        "    !rm -r {image_dir_out}.ipynb_checkpoints\n"
      ],
      "metadata": {
        "id": "T-5VcETfQAR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN 2** "
      ],
      "metadata": {
        "id": "vtbymxQuOcXW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3-r9u0POcXX"
      },
      "source": [
        "**Model and training code**\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODING BLOCK** "
      ],
      "metadata": {
        "id": "Ya1v16NLOcXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "WHJKNAuvOcXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP MODEL DEFINATION**\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "Wfdc8fraOcXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -qq -U flax orbax\n",
        "# Orbax needs to enable asyncio in a Colab environment.\n",
        "!python -m pip install -qq nest_asyncio\n",
        "\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        for i in range(ndl):\n",
        "            x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "fBWJpWQ-OcXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initialize the module**"
      ],
      "metadata": {
        "id": "Uy5pjMvjOcXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def Create_train_state(r_key, model, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    variables = model.init(r_key, jnp.ones(shape)) \n",
        "    optimizer = optax.adam(learning_rate) \n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "qlst_jMhOcXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining loss function**"
      ],
      "metadata": {
        "id": "nk7vrtd1OcXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#serial\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2) \n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  metrics = {\n",
        "      'loss': loss,     #LOSS\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "BdL-XGU2OcXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train step defination**"
      ],
      "metadata": {
        "id": "YM0w1GDhOcXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cpu serial\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    image, label = batch  \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);\n",
        "        loss =  image_difference_loss(logits, label);\n",
        "        return loss, logits\n",
        "\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = gradient_fn(state.params)\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    logs = compute_metrics(logits=logits, labels=label)\n",
        "    return new_state, logs\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, image):\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=image)\n"
      ],
      "metadata": {
        "id": "ahpb0DxBOcXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image conversion fiunctions**"
      ],
      "metadata": {
        "id": "wHKiqTxCOcXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu"
      ],
      "metadata": {
        "id": "rxbefKzwOcXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image dataset, image size and batch size Setup**"
      ],
      "metadata": {
        "id": "plbG5Hx5OcXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## to load the data in batches as mentioned single batch of images with already provided sizes \n",
        "import jax\n",
        "from jax import random\n",
        "\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "batch_size = 1\n",
        "\n",
        "import os\n",
        "image_dir = r'/content/MEDIAPIPEinput'\n",
        "annotated_image_dir = r'/content/annotated_images'\n",
        "\n",
        "#############################################################################\n",
        "bandend = [\"c\",\".png\", \"b02\"]\n",
        "expression_b2 = bandend[1]\n",
        "\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "\n",
        "annotated_total_images =  [f for f in os.listdir(annotated_image_dir) if f.__contains__(expression_b2)]\n",
        "annotated_total_images.sort()\n",
        "annotated_total_images_path = [os.path.join(annotated_image_dir, i) for i in annotated_total_images if i != 'outputs']\n",
        "\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "\n",
        "######################################## making 8 array of input for each device >>>\n",
        "def batchedimages(image_locations):\n",
        "  RGB8 = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1]))\n",
        "  ANNOTATED8 = jnp.asarray((imageRGB(annotated_total_images_path[image_locations[0]])[1]))\n",
        "  batch_ccc = RGB8, ANNOTATED8 \n",
        "  return batch_ccc\n",
        "\n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  "
      ],
      "metadata": {
        "id": "qjCNWEP9OcXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(batches)[0].shape"
      ],
      "metadata": {
        "id": "PUh2xVp4S6ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **ðŸ‘ HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from google.colab import output\n",
        "import orbax.checkpoint as orbax\n",
        "from flax.training import checkpoints\n",
        "\n",
        "import optax\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "CKPT_DIR = 'ckpts'\n",
        "\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  HxW, Channels = next(batches)[0].shape\n",
        "  state = Create_train_state( rng, model, (HxW, Channels), learning_rate ) \n",
        "  count = 1\n",
        "#âœ…âœ…ðŸ”» state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "###################### \n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "\n",
        "total_epochs = 50\n",
        "for epochs in tqdm(range(total_epochs)):  \n",
        "  batches = data_stream() \n",
        "\n",
        "  if checkpoint_available:\n",
        "    state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r {dir}\n",
        "\n",
        "  for bbb in tqdm(range(no_of_batches-5)):\n",
        "    state, metrics = train_step(state, next(batches), rng)\n",
        "    output.clear()\n",
        "    print(\"loss: \",metrics['loss'],\" <<< \") # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step=epochs, prefix='checkpoint_', keep=1, overwrite=False, orbax_checkpointer=orbax_checkpointer)\n",
        "  # restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "\n"
      ],
      "metadata": {
        "id": "LakSOVw3OcXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inference engine**"
      ],
      "metadata": {
        "id": "iE4IPvggOcXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np \n",
        "from google.colab import output\n",
        "\n",
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "image_in = '/content/a.jpg'\n",
        "\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "image = jnp.asarray((imageRGB(image_in)[1]))\n",
        "#restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "#state = restored_state\n",
        "prediction = eval_step(state, image)\n",
        "prediction['loss']\n",
        "\n",
        "\n",
        "predicted_image = np.array(prediction['logits'],  dtype=np.uint8).reshape(newsize) \n",
        "cv2_imshow(predicted_image)\n"
      ],
      "metadata": {
        "id": "VAmef9UcOcXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import numpy as np \n",
        "# def show_image(argu):\n",
        "#   L1 = argu[0]\n",
        "#   predicted_image = np.array(argu[0],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "#   a = predicted_image\n",
        "#   for i in range(0,argu.shape[0]):\n",
        "#     predicted_image = np.array(argu[i],  dtype=np.uint8).reshape(newsize) \n",
        "#     a = cv2.hconcat([a, predicted_image])\n",
        "#   cv2_imshow(a)\n",
        "\n",
        "# show_image(metrics['logits'])"
      ],
      "metadata": {
        "id": "ejRwflrdOcXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from jax.tree_util import tree_structure\n",
        "# print(tree_structure(state))"
      ],
      "metadata": {
        "id": "VYrnNTEpOcXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ensemble**"
      ],
      "metadata": {
        "id": "ka3_468K9xRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "from typing import Any\n",
        "import jax\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "import functools\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        for i in range(ndl):\n",
        "            x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "\n",
        "@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2))\n",
        "def Create_train_state(r_key, shape, learning_rate ):\n",
        "    print(shape)\n",
        "    model = MLPModel()\n",
        "    variables = model.init(r_key, jnp.ones(shape)) \n",
        "    optimizer = optax.adam(learning_rate) \n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "# learning_rate = 1e-4\n",
        "# batch_size_no = 64\n",
        "\n",
        "# model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "Aaat0R0q9Z7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.pmap, axis_name='ensemble')\n",
        "def apply_model(state, batch: jnp.asarray):\n",
        "  image, label = batch\n",
        "  def loss_fn(params):\n",
        "    logits = MLPModel().apply({'params': params}, image)\n",
        "    loss =  image_difference_loss(logits, label);\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  return grads, loss\n",
        "\n",
        "@jax.pmap\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "id": "QyGP4Fmf-q7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(state, train_ds, batch_size, rng):\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "\n",
        "  for perm in perms:\n",
        "    batch_images = flax.jax_utils.replicate(train_ds['image'][perm, ...])\n",
        "    batch_labels = flax.jax_utils.replicate(train_ds['label'][perm, ...])\n",
        "    grads, loss = apply_model(state, batch_images, batch_labels)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(flax.jax_utils.unreplicate(loss))\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  return state, train_loss"
      ],
      "metadata": {
        "id": "QQUj3Y3LA9A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = get_datasets()\n",
        "test_ds = flax.jax_utils.replicate(test_ds)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "\n",
        "HxW, Channels = next(batches)[0].shape\n",
        "state = create_train_state(jax.random.split(init_rng, jax.device_count()),(HxW, Channels),learning_rate)\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  rng, input_rng = jax.random.split(rng)\n",
        "  state, train_loss = train_epoch(state, train_ds, batch_size, input_rng)\n",
        "\n",
        "  # _, test_loss = flax.jax_utils.unreplicate(apply_model(state, test_ds['image'], test_ds['label']))\n",
        "\n",
        "  logging.info('epoch:% 3d, train_loss: %.4f ' % (epoch, train_loss))"
      ],
      "metadata": {
        "id": "X-CttLscBnDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same as before, but using @pad_shard_unshard decorator\n",
        "\n",
        "# manually padding\n",
        "# => precise & allows for data parallelism\n",
        "\n",
        "@jax.pmap\n",
        "def get_preds(variables, inputs):\n",
        "  print('retrigger compilation', inputs.shape)\n",
        "  return model.apply(variables, inputs)\n",
        "\n",
        "ds = tfds.load(dataset_name, split=tfds.split_for_jax_process('test'))\n",
        "per_host_batch_size = per_device_batch_size * jax.local_device_count()\n",
        "ds = ds.batch(per_host_batch_size, drop_remainder=False)\n",
        "\n",
        "correct = total = 0\n",
        "for batch in ds.as_numpy_iterator():\n",
        "  preds = flax.jax_utils.pad_shard_unpad(get_preds)(\n",
        "      vs, batch['image'], min_device_batch=per_device_batch_size)\n",
        "  total += len(batch['image'])\n",
        "  correct += (batch['label'] == preds.argmax(axis=-1)).sum()\n",
        "\n",
        "correct = correct.item()\n",
        "correct, total, correct / total"
      ],
      "metadata": {
        "id": "I_orMqbuD3LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(metrics, variables, batch):\n",
        "  print('retrigger compilation', {k: v.shape for k, v in batch.items()})\n",
        "  preds = model.apply(variables, batch['image'])\n",
        "  correct = (batch['mask'] & (batch['label'] == preds.argmax(axis=-1))).sum()\n",
        "  total = batch['mask'].sum()\n",
        "  return dict(\n",
        "      correct=metrics['correct'] + jax.lax.psum(correct, axis_name='batch'),\n",
        "      total=metrics['total'] + jax.lax.psum(total, axis_name='batch'),\n",
        "  )\n",
        "\n",
        "eval_step = jax.pmap(eval_step, axis_name='batch')\n",
        "eval_step = flax.jax_utils.pad_shard_unpad(\n",
        "    eval_step, static_argnums=(0, 1), static_return=True)"
      ],
      "metadata": {
        "id": "RxhJjRZLD5P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {total_files}\n",
        "%cd ..\n",
        "!zip -r folder.zip {total_files}"
      ],
      "metadata": {
        "id": "9PboYgRwzhb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/folder.zip /content/drive/MyDrive/OUT/data/machine_learning_test_dataset"
      ],
      "metadata": {
        "id": "N_Mbj-WH0raq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip"
      ],
      "metadata": {
        "id": "dF77PT3FHZ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/s.zip #unzipping the flower images from archive.."
      ],
      "metadata": {
        "id": "vImC4C6oHeRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_files= '/content/s'\n",
        "input_images = '/content/MEDIAPIPEinput'\n",
        "out_images = '/content/annotated_images'\n",
        "!mkdir -p {total_files}\n",
        "!cp -r {input_images} {total_files}\n",
        "!cp -r {out_images} {total_files}"
      ],
      "metadata": {
        "id": "FsMi1Ce8xX0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {total_files}\n",
        "!tfds new my_dataset"
      ],
      "metadata": {
        "id": "vRFhGL28yCvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_files= '/content/s'\n",
        "%cd {total_files}/my_dataset/\n",
        "!tfds build"
      ],
      "metadata": {
        "id": "KX-D3ii8yRFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/t/my_dataset"
      ],
      "metadata": {
        "id": "0ODocGRg82Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def _generate_examples(self, path):\n",
        "  \"\"\"Yields examples.\"\"\"\n",
        "  # TODO(my_dataset): Yields (key, example) tuples from the dataset\n",
        "  for f in path.glob('*.png'):\n",
        "    yield 'key', {\n",
        "        'MEDIAPIPEinput': f,\n",
        "        'annotated_images': f,\n",
        "    }\n",
        "    \n",
        "os.path = r'/content/s'\n",
        "_generate_examples(path / 'MEDIAPIPEinput')"
      ],
      "metadata": {
        "id": "oEUwsD8Q4uS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "dl_manager = tfds.download.DownloadManager(download_dir='/content')\n",
        "urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "path = dl_manager.extract(dl_manager.download(urls))"
      ],
      "metadata": {
        "id": "-2d0H4Yh97iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_examples( img_path):\n",
        "  # Read the input data out of the source files\n",
        "  # with img_path.open() as f:\n",
        "    yield {\n",
        "        'image': img_path / '*.png',\n",
        "    }\n",
        "\n",
        "def _split_generators():\n",
        "    \"\"\"Download the data and define splits.\"\"\"\n",
        "    dl_manager = tfds.download.DownloadManager(download_dir='/content')\n",
        "    urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "    path = dl_manager.extract(dl_manager.download(urls))    # dl_manager returns pathlib-like objects with `path.read_text()`,\n",
        "    # `path.iterdir()`,...\n",
        "    return {\n",
        "        'in_image': _generate_examples(path / 'MEDIAPIPEinput'),\n",
        "        'out_image': _generate_examples(path / 'annotated_images'),\n",
        "    }\n",
        "# _generate_examples(path/'MEDIAPIPEinput')\n",
        "print(_split_generators()['in_image'])"
      ],
      "metadata": {
        "id": "Q7fq5IunA5-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "str(path)"
      ],
      "metadata": {
        "id": "QkHMo5itRsIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(_generate_examples(path / 'MEDIAPIPEinput'))['image']"
      ],
      "metadata": {
        "id": "SnxXUYPSX2Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Builder(tfds.core.GeneratorBasedBuilder):\n",
        "  \"\"\"DatasetBuilder for my_dataset dataset.\"\"\"\n",
        "\n",
        "  VERSION = tfds.core.Version('1.0.0')\n",
        "  RELEASE_NOTES = {\n",
        "      '1.0.0': 'Initial release.',\n",
        "  }\n",
        "\n",
        "  def _info(self) -> tfds.core.DatasetInfo:\n",
        "    \"\"\"Dataset metadata (homepage, citation,...).\"\"\"\n",
        "    return self.dataset_info_from_configs(\n",
        "        features=tfds.features.FeaturesDict({\n",
        "            'image': tfds.features.Image(shape=(256, 256, 3)),\n",
        "            'label': tfds.features.Image(shape=(256, 256, 3)),\n",
        "        }),\n",
        "    )\n",
        "\n",
        "  def _split_generators(self, dl_manager: tfds.download.DownloadManager):\n",
        "    \"\"\"Download the data and define splits.\"\"\"\n",
        "    urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "    extracted_path = dl_manager.download_and_extract(urls)\n",
        "    # dl_manager returns pathlib-like objects with `path.read_text()`,\n",
        "    # `path.iterdir()`,...\n",
        "    return {\n",
        "        'train': self._generate_input_examples(path=extracted_path / 'MEDIAPIPEinput'),\n",
        "        'test': self._generate_output_examples(path=extracted_path / 'annotated_images'),\n",
        "    }\n",
        "\n",
        "  def _generate_input_examples(self, path) -> Iterator[Tuple[Key, Example]]:\n",
        "    \"\"\"Generator of examples for each split.\"\"\"\n",
        "    for img_path in path.glob('*.png'):\n",
        "      # Yields (key, example)\n",
        "      yield img_path.name, {\n",
        "          'image': img_path,\n",
        "      }\n",
        "  def _generate_output_examples(self, path) -> Iterator[Tuple[Key, Example]]:\n",
        "    \"\"\"Generator of examples for each split.\"\"\"\n",
        "    for img_path in path.glob('*.png'):\n",
        "      # Yields (key, example)\n",
        "      yield img_path.name, {\n",
        "          'image': img_path,\n",
        "      }"
      ],
      "metadata": {
        "id": "inwnL9-tGNT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the modules\n",
        "import os\n",
        "from os import listdir\n",
        " \n",
        "# get the path or directory\n",
        "folder_dir = str(path)+'/MEDIAPIPEinput/'\n",
        "for images in os.listdir(folder_dir):\n",
        " \n",
        "    # check if the image ends with png or jpg or jpeg\n",
        "    if (images.endswith(\".png\") or images.endswith(\".jpg\") or images.endswith(\".jpeg\")):\n",
        "        # display\n",
        "        print(images)"
      ],
      "metadata": {
        "id": "79xoNImGLVMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path"
      ],
      "metadata": {
        "id": "lzos6UZeNq2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.keras.utils.image_dataset_from_directory(\n",
        "  path,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "OVXCLpCENKMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "sOAIMTU_NwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "data_dir = tf.keras.utils.get_file(origin=urls,\n",
        "                                   fname='s',\n",
        "                                   cache_subdir='/content/biy',\n",
        "                                   archive_format='auto',\n",
        "                                   untar=False,\n",
        "                                   extract=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "UpkVRTg_RNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/biy"
      ],
      "metadata": {
        "id": "CPhcP4p1hx-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir"
      ],
      "metadata": {
        "id": "x7WkVlFGRjDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_count = len(list(data_dir.glob('*/*.png')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "emZOpEFHRf1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import flax.linen as nn\n",
        "import jax.numpy as jnp\n",
        "from jax.random import PRNGKey\n",
        "\n",
        "x = jnp.empty((4, 28, 28, 1)) \n",
        "\n",
        "x.reshape((x.shape[0], -1)).shape\n",
        "\n",
        "class MLP(nn.Module):                              # create a Flax Module dataclass\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, input_points):\n",
        "    # x = x.reshape((x.shape[0], -1))\n",
        "    # x = nn.Dense(128)(x)                           # create inline Flax Module submodules\n",
        "    # x = nn.relu(x)\n",
        "    # x = nn.Dense(1)(x)                 # shape inference\n",
        "    # return x\n",
        "    for i in range(8):\n",
        "      x = nn.Dense(256)(x)\n",
        "      x = nn.relu(x)\n",
        "      x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(1)(x)\n",
        "      return x\n",
        "model = MLP()                           # instantiate the MLP model\n",
        "\n",
        "x = jnp.empty((4, 28, 28, 1))                      # generate random data\n",
        "params = model.init(PRNGKey(42), x)[\"params\"]      # initialize the weights\n",
        "y = model.apply({\"params\":params}, x)  "
      ],
      "metadata": {
        "id": "o4MNU3XHpEhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "id": "227DGBDgp5Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "x = jnp.empty((4, 28, 28, 1))\n",
        "positional_encoding(x.reshape(x.shape[0],-1)).shape"
      ],
      "metadata": {
        "id": "6yyDrzoqrpT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "PATH = '/content/biy/'\n",
        "BATCH_SIZE = 12\n",
        "IMAGE_SIZE = 140\n",
        "\n",
        "def read_train_data():\n",
        "    x_files = [f for f in glob.glob(PATH + \"MEDIAPIPEinput/*.png\", recursive=True)]\n",
        "    y_files = [f for f in glob.glob(PATH + \"annotated_images/*.png\", recursive=True)]\n",
        "\n",
        "    def read_image(x_filename, y_filename):\n",
        "        x_image_string = tf.io.read_file(x_filename)\n",
        "        y_image_string = tf.io.read_file(y_filename)\n",
        "\n",
        "        x_image_decoded = tf.image.decode_jpeg(x_image_string, channels=3)\n",
        "        y_image_decoded = tf.image.decode_jpeg(y_image_string, channels=3)\n",
        "\n",
        "        x_image_resized = tf.image.resize(x_image_decoded, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "        y_image_resized = tf.image.resize(y_image_decoded, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "\n",
        "        x_image_norm = x_image_resized / 255\n",
        "        y_image_norm = y_image_resized / 255\n",
        "\n",
        "        return x_image_norm, y_image_norm\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_files, y_files))\n",
        "\n",
        "    dataset = dataset.map(read_image).shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "train_set = read_train_data()\n",
        "for x, y in train_set.as_numpy_iterator():\n",
        "  print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "Z8UdlS_531XQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install -q -U flax\n",
        "import functools\n",
        "from flax.training.train_state import TrainState\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = jnp.ravel(y_true)\n",
        "    y_pred = jnp.ravel(y_pred)\n",
        "    intersection = jnp.sum(y_true * y_pred)\n",
        "    return 2.0 * intersection / (jnp.sum(y_true) + jnp.sum(y_pred) + 1)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "\n",
        "class CustomTrainState(TrainState):\n",
        "    def apply_fn_with_bn(self, *args, is_training, **nargs):\n",
        "        output = self.apply_fn(*args, **nargs,rngs={'dropout': jax.random.PRNGKey(2)})\n",
        "        return output\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3,))\n",
        "def train_step(x, y, train_state, is_training=True):\n",
        "    def loss_fn(params, is_training):\n",
        "        y_pred= train_state.apply_fn_with_bn({\"params\": params}, x, is_training=is_training)\n",
        "        loss = dice_coef_loss(y, y_pred)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    if is_training:\n",
        "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "        (loss), grads = grad_fn(train_state.params, True)\n",
        "\n",
        "        train_state = train_state.apply_gradients(grads=grads)\n",
        "    else:\n",
        "        loss = loss_fn(train_state.params, False)\n",
        "\n",
        "    return loss, train_state"
      ],
      "metadata": {
        "id": "OYapBRMtyKdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "unet = MLP(out_dims=10)\n",
        "\n",
        "init_rngs = {'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(1)}\n",
        "\n",
        "unet_variables = unet.init(init_rngs, jnp.ones([1, IMAGE_SIZE, IMAGE_SIZE, 3]))\n",
        "\n",
        "optimizer = optax.adam(learning_rate=0.001)\n",
        "\n",
        "train_state = CustomTrainState.create(apply_fn=unet.apply, params=unet_variables[\"params\"], tx=optimizer)\n",
        "\n",
        "\n",
        "for e in range(20):\n",
        "        loss_avg = 0\n",
        "        for x, y in train_set.as_numpy_iterator():\n",
        "            loss, train_state = train_step(x, y, train_state, True)\n",
        "            print(f\"epoch: {e}, loss: {loss:0.2f}\")"
      ],
      "metadata": {
        "id": "xQJtLnKfymB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.empty((4, 28, 28, 3))\n",
        "c = x[2]\n",
        "c.shape\n",
        "c = c.reshape(-1, c.shape[2])\n",
        "p = positional_encoding(c)\n",
        "print(p.shape)"
      ],
      "metadata": {
        "id": "qH1bBmY0eVy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "x = jnp.empty((4, 28, 28, 1))\n",
        "print(x.shape)\n",
        "\n",
        "def positional_encoding(args):\n",
        "    print(args.shape)\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(6))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "    \n",
        "img_list = []\n",
        "for i in range(x.shape[0]):\n",
        "  print(i)\n",
        "  print(x[i].shape)\n",
        "  c = x[i]\n",
        "  c.shape\n",
        "  c = c.reshape(-1, c.shape[2])\n",
        "  p = positional_encoding(c)\n",
        "  img_list.append(p)\n",
        "  print(p.shape)"
      ],
      "metadata": {
        "id": "EB5Rv3cPa1kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape, jnp.array(img_list).shape)"
      ],
      "metadata": {
        "id": "WxgiDRogqsij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**starting here ðŸ”»**"
      ],
      "metadata": {
        "id": "jUx9EzYmw_3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "import flax.linen as nn\n",
        "from typing import Any\n",
        "import jax\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "def batch_encoded(args):\n",
        "    img_list = []\n",
        "    for i in range(args.shape[0]):\n",
        "        c = args[i]\n",
        "        c = c.reshape(-1, c.shape[2])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(p.reshape(args.shape[1],args.shape[2],p.shape[1]))\n",
        "        x = jnp.array(img_list)\n",
        "    return x\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "      x = batch_encoded(input_points) if self.apply_positional_encoding else input_points\n",
        "      for i in range(ndl):\n",
        "          x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "          x = nn.relu(x)\n",
        "          x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(3, dtype=self.dtype, precision=self.precision)(x)\n",
        "      return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "GKR8uycsszAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "unet = MLPModel()\n",
        "\n",
        "init_rngs = {'params': jax.random.PRNGKey(0), 'dropout': jax.random.PRNGKey(1)}\n",
        "IMAGE_SIZE = 140\n",
        "unet_variables = unet.init(init_rngs, jnp.ones([7, IMAGE_SIZE, IMAGE_SIZE, 3]))\n",
        "\n",
        "optimizer = optax.adam(learning_rate=0.001)\n",
        "\n",
        "from flax.training.train_state import TrainState\n",
        "\n",
        "class CustomTrainState(TrainState):\n",
        "    def apply_fn_with_bn(self, *args, is_training, **nargs):\n",
        "        output = self.apply_fn(*args, **nargs,rngs={'dropout': jax.random.PRNGKey(2)})\n",
        "        return output\n",
        "\n",
        "train_state = CustomTrainState.create(apply_fn=unet.apply, params=unet_variables[\"params\"], tx=optimizer)"
      ],
      "metadata": {
        "id": "dYWCevDSuVVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.tree_util import tree_structure\n",
        "print(tree_structure(train_state))"
      ],
      "metadata": {
        "id": "E95IUYDTNHC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "data_dir = tf.keras.utils.get_file(origin=urls,\n",
        "                                   fname='s',\n",
        "                                   cache_subdir='/content/biy',\n",
        "                                   archive_format='auto',\n",
        "                                   untar=False,\n",
        "                                   extract=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "cMAduDgwPfaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3,))\n",
        "def train_step(x, y, train_state, is_training=True):\n",
        "    def loss_fn(params, is_training):\n",
        "        y_pred= train_state.apply_fn_with_bn({\"params\": params}, x, is_training=is_training)\n",
        "        loss = dice_coef_loss(y, y_pred)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    if is_training:\n",
        "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "        loss, grads = grad_fn(train_state.params, True)\n",
        "\n",
        "        train_state = train_state.apply_gradients(grads=grads)\n",
        "    else:\n",
        "        loss = loss_fn(train_state.params, False)\n",
        "\n",
        "    return loss, train_state\n",
        "\n",
        "PATH = '/content/biy/'\n",
        "BATCH_SIZE = 12\n",
        "IMAGE_SIZE = 140\n",
        "\n",
        "def read_train_data():\n",
        "    x_files = [f for f in glob.glob(PATH + \"MEDIAPIPEinput/*.png\", recursive=True)]\n",
        "    y_files = [f for f in glob.glob(PATH + \"annotated_images/*.png\", recursive=True)]\n",
        "\n",
        "    def read_image(x_filename, y_filename):\n",
        "        x_image_string = tf.io.read_file(x_filename)\n",
        "        y_image_string = tf.io.read_file(y_filename)\n",
        "\n",
        "        x_image_decoded = tf.image.decode_png(x_image_string, channels=3)\n",
        "        y_image_decoded = tf.image.decode_png(y_image_string, channels=3)\n",
        "\n",
        "        x_image_resized = tf.image.resize(x_image_decoded, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "        y_image_resized = tf.image.resize(y_image_decoded, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "\n",
        "        x_image_norm = x_image_resized / 255\n",
        "        y_image_norm = y_image_resized / 255\n",
        "\n",
        "        return x_image_norm, y_image_norm\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((x_files, y_files))\n",
        "\n",
        "    dataset = dataset.map(read_image).shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4k8J2h_QwMtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = read_train_data()\n",
        "\n",
        "\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true = jnp.ravel(y_true)\n",
        "    y_pred = jnp.ravel(y_pred)\n",
        "    intersection = jnp.sum(y_true * y_pred)\n",
        "    return 2.0 * intersection / (jnp.sum(y_true) + jnp.sum(y_pred) + 1)\n",
        "\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    return 1.0 - dice_coef(y_true, y_pred)\n",
        "\n",
        "for e in range(20):\n",
        "        loss_avg = 0\n",
        "        for x, y in train_set.as_numpy_iterator():\n",
        "            loss, train_state = train_step(x, y, train_state, True)\n",
        "            print(f\"epoch: {e}, loss: {loss:0.2f}\")"
      ],
      "metadata": {
        "id": "Jefe9HxKxO-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**ensemble test**"
      ],
      "metadata": {
        "id": "k1KLX-FNqSFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "data_dir = tf.keras.utils.get_file(origin=urls,\n",
        "                                   fname='s',\n",
        "                                   cache_subdir='/content/biy',\n",
        "                                   archive_format='auto',\n",
        "                                   untar=False,\n",
        "                                   extract=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "id": "1dyRUFLWq0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "from typing import Any\n",
        "import jax\n",
        "from jax import lax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "import functools\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "def batch_encoded(args):\n",
        "    img_list = []\n",
        "    for i in range(args.shape[0]):\n",
        "        c = args[i]\n",
        "        c = c.reshape(-1, c.shape[2])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(p.reshape(args.shape[1],args.shape[2],p.shape[1]))\n",
        "        x = jnp.array(img_list)\n",
        "    return x\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "      x = batch_encoded(input_points) if self.apply_positional_encoding else input_points\n",
        "      for i in range(ndl):\n",
        "          x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "          x = nn.relu(x)\n",
        "          x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(3, dtype=self.dtype, precision=self.precision)(x)\n",
        "      return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "\n",
        "@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2))\n",
        "def Create_train_state(r_key, shape, learning_rate ):\n",
        "    print(shape)\n",
        "    model = MLPModel()\n",
        "    variables = model.init(r_key, jnp.ones(shape)) \n",
        "    optimizer = optax.adam(learning_rate) \n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "# learning_rate = 1e-4\n",
        "# batch_size_no = 64\n",
        "\n",
        "# model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "V7oQoK4fqWXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPModel()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "rn_g, init_rng = jax.random.split(rng)\n",
        "model.init(rng, jnp.ones(next(data_stream())[0].shape)) "
      ],
      "metadata": {
        "id": "6hzg1QbcnkGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@functools.partial(jax.pmap, axis_name='batch')\n",
        "def apply_model(state, batch: jnp.asarray):\n",
        "  image, label = batch\n",
        "  def loss_fn(params):\n",
        "    logits = MLPModel().apply({'params': params}, image)\n",
        "    loss =  image_difference_loss(logits, label);\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  return grads, loss\n",
        "\n",
        "@jax.pmap\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "id": "eOKwXeVSqWXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(state, train_ds, rng):\n",
        "  train_ds_size = train_ds.shape[0]\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, train_ds.shape[0])\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "\n",
        "  for perm in perms:\n",
        "    x_images = flax.jax_utils.replicate(train_ds[perm, ...])\n",
        "    grads, loss = apply_model(state, train_ds)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(flax.jax_utils.unreplicate(loss))\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  return state, train_loss"
      ],
      "metadata": {
        "id": "WfDPh1VVqWXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## to load the data in batches as mentioned single batch of images with already provided sizes \n",
        "import jax\n",
        "from jax import random\n",
        "\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "batch_size = 10\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "\n",
        "x_image_dir = r'/content/biy/MEDIAPIPEinput/'\n",
        "y_image_dir = r'/content/biy/annotated_images/'\n",
        "\n",
        "#############################################################################\n",
        "bandend = [\"c\",\".png\", \"b02\"]\n",
        "expression_b2 = bandend[1]\n",
        "x_total_images =  [f for f in os.listdir(x_image_dir) if f.__contains__(expression_b2)]\n",
        "x_total_images.sort()\n",
        "x_total_images_path = [os.path.join(x_image_dir, i) for i in x_total_images if i != 'outputs']\n",
        "no_of_batches = int(len(x_total_images_path)/batch_size)\n",
        "\n",
        "\n",
        "y_total_images =  [f for f in os.listdir(y_image_dir) if f.__contains__(expression_b2)]\n",
        "y_total_images.sort()\n",
        "y_total_images_path = [os.path.join(y_image_dir, i) for i in y_total_images if i != 'outputs']\n",
        "\n",
        "\n",
        "######################################## making 8 array of input for each device >>>\n",
        "def batchedimages(total_images_path, image_locations):\n",
        "  RGB8 = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[0]))\n",
        "  return RGB8\n",
        "\n",
        "\n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  x_image_list = []\n",
        "  y_image_list = []\n",
        "\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    x_image_list.append(batchedimages(x_total_images_path, batch_idx))\n",
        "    y_image_list.append(batchedimages(y_total_images_path, batch_idx))\n",
        "  yield jnp.array(x_image_list),jnp.array(y_image_list)\n",
        "\n",
        "batches = data_stream()  \n"
      ],
      "metadata": {
        "id": "JUbPb0MMR7Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = next(data_stream())\n",
        "# test_ds = jax_utils.replicate(test_ds)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "rn_g, init_rng = jax.random.split(rng)\n",
        "\n",
        "BATCH, H, W, Channels = next(data_stream())[0].shape\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# state = Create_train_state(jax.random.split(init_rng, jax.device_count()),(BATCH, H, W, Channels),learning_rate)\n",
        "state = Create_train_state(jax.random.split(init_rng, 1),(next(data_stream())[0].shape),learning_rate)\n",
        "num_epochs = 3\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  rng, input_rng = jax.random.split(rng)\n",
        "  state, train_loss = train_epoch(state, train_ds, input_rng)\n",
        "\n",
        "  # _, test_loss = jax_utils.unreplicate(apply_model(state, test_ds['image'], test_ds['label']))\n",
        "\n",
        "  logging.info('epoch:% 3d, train_loss: %.4f ' % (epoch, train_loss))"
      ],
      "metadata": {
        "id": "S6qpssQEqWXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same as before, but using @pad_shard_unshard decorator\n",
        "\n",
        "# manually padding\n",
        "# => precise & allows for data parallelism\n",
        "\n",
        "@jax.pmap\n",
        "def get_preds(variables, inputs):\n",
        "  print('retrigger compilation', inputs.shape)\n",
        "  return model.apply(variables, inputs)\n",
        "\n",
        "ds = tfds.load(dataset_name, split=tfds.split_for_jax_process('test'))\n",
        "per_host_batch_size = per_device_batch_size * jax.local_device_count()\n",
        "ds = ds.batch(per_host_batch_size, drop_remainder=False)\n",
        "\n",
        "correct = total = 0\n",
        "for batch in ds.as_numpy_iterator():\n",
        "  preds = flax.jax_utils.pad_shard_unpad(get_preds)(\n",
        "      vs, batch['image'], min_device_batch=per_device_batch_size)\n",
        "  total += len(batch['image'])\n",
        "  correct += (batch['label'] == preds.argmax(axis=-1)).sum()\n",
        "\n",
        "correct = correct.item()\n",
        "correct, total, correct / total"
      ],
      "metadata": {
        "id": "slrmR8coqWXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ds, y_train_ds = next(data_stream())\n",
        "print(x_train_ds.shape, y_train_ds.shape)"
      ],
      "metadata": {
        "id": "Kut8CGgKcMKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ds[0].shape\n"
      ],
      "metadata": {
        "id": "L_3E7rPlfBGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as onp\n",
        "img = onp.array(x_train_ds[0])\n",
        "cv2_imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
      ],
      "metadata": {
        "id": "g1eZPtjUfNTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from flax import linen as nn\n",
        "from flax import jax_utils\n",
        "import optax\n",
        "from flax.training.train_state import TrainState\n",
        "\n",
        "model = MLPModel()\n",
        "x = jnp.ones((2,100, 100, 3))\n",
        "params = model.init(jax.random.PRNGKey(0), x)\n",
        "tx = optax.adam(learning_rate=1e-3)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx,)\n",
        "state = jax_utils.replicate(state)\n",
        "\n",
        "def loss_fn(state, x):\n",
        "    return (model.apply(state.params, x) ** 2.0).mean()\n",
        "\n",
        "jax.pmap(loss_fn)(state, x)"
      ],
      "metadata": {
        "id": "dyNs4zjh8zv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing model pmap *bug"
      ],
      "metadata": {
        "id": "edh13ga6-mS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "def batch_encoded(args):\n",
        "    img_list = []\n",
        "    for i in range(args.shape[0]):\n",
        "        c = args[i]\n",
        "        c = c.reshape(-1, c.shape[2])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(p.reshape(args.shape[1],args.shape[2],p.shape[1]))\n",
        "        x = jnp.array(img_list)\n",
        "    return x\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "      x = batch_encoded(input_points) if self.apply_positional_encoding else input_points\n",
        "      for i in range(ndl):\n",
        "          x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "          x = nn.relu(x)\n",
        "          x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "      return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "id": "dGiEMFZ_-qPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.tree_util import tree_structure\n",
        "print(tree_structure(state.params))"
      ],
      "metadata": {
        "id": "hAlM3xv1_RNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9TPWln0UB2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **RUN 2 testing** "
      ],
      "metadata": {
        "id": "OOUqoXxBUCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "urls = 'https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip'\n",
        "data_dir = tf.keras.utils.get_file(origin=urls,\n",
        "                                   fname='s',\n",
        "                                   cache_subdir='/content/',\n",
        "                                   archive_format='auto',\n",
        "                                   untar=False,\n",
        "                                   extract=True)\n",
        "data_dir = pathlib.Path(data_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AJIkPwnV_sO",
        "outputId": "b47fceff-a1c7-4d86-af07-2f9d68a6d391"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/s.zip\n",
            "821864248/821864248 [==============================] - 155s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjgCYS3gUCL3"
      },
      "source": [
        "**Model and training code**\n",
        "Our model is a coordinate-based multilayer perceptron. In this example, for each input image coordinate $(x,y)$, the model predicts the associated color $(r,g,b)$ or any $(gray)$.\n",
        "\n",
        "![Network diagram](https://user-images.githubusercontent.com/3310961/85066930-ad444580-b164-11ea-9cc0-17494679e71f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**POSITIONAL ENCODING BLOCK** "
      ],
      "metadata": {
        "id": "iIR4yJ1DUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(args):\n",
        "    image_height_x_image_width, cha = args.shape\n",
        "    inputs_freq = jax.vmap(lambda x: args * 2.0 ** x)(jnp.arange(positional_encoding_dims))\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)])\n",
        "    x = x.swapaxes(0, 2)\n",
        "    x = x.reshape([image_height_x_image_width, -1])\n",
        "    x = jnp.concatenate([args, x], axis=-1)\n",
        "    return x\n",
        "\n",
        "def batch_encoded(args):\n",
        "    img_list = []\n",
        "    for i in range(args.shape[0]):\n",
        "        c = args[i]\n",
        "        c = c.reshape(-1, c.shape[2])\n",
        "        p = positional_encoding(c)\n",
        "        img_list.append(p.reshape(args.shape[1],args.shape[2],p.shape[1]))\n",
        "        x = jnp.array(img_list)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "A9QJZJ50UCL3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP MODEL DEFINATION**\n",
        "Basically, passing input points through a simple Fourier Feature Mapping enables an MLP to learn high-frequency functions (such as an RGB image) in low-dimensional problem domains (such as a 2D coordinate of pixels)."
      ],
      "metadata": {
        "id": "AicbTZ7jUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -qq -U flax orbax\n",
        "# Orbax needs to enable asyncio in a Colab environment.\n",
        "!python -m pip install -qq nest_asyncio\n",
        "\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state, common_utils\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "ndl = 8 # num_dense_layers Number of dense layers in MLP\n",
        "dlw = 256 # dense_layer_width Dimentionality of dense layers' output space \n",
        "\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "      x = batch_encoded(input_points) if self.apply_positional_encoding else input_points\n",
        "      for i in range(ndl):\n",
        "          x = nn.Dense(dlw,dtype=self.dtype,precision=self.precision)(x)\n",
        "          x = nn.relu(x)\n",
        "          x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "      x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "      return x\n",
        "##########################################<< MLP MODEL >>#########################################"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLuo6LGHUCL3",
        "outputId": "ac225ea7-cb8e-4f9f-ad6f-7f2217b343da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 197 kB 27.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66 kB 5.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154 kB 54.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 238 kB 72.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.3 MB 52.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51 kB 5.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85 kB 4.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**initialize the module**"
      ],
      "metadata": {
        "id": "xGaXMyhpUCL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#âœ…\n",
        "!python -m pip install -q -U flax\n",
        "import optax\n",
        "from flax.training import train_state\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "\n",
        "def Create_train_state(r_key, model, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    variables = model.init(r_key, jnp.ones(shape)) \n",
        "    optimizer = optax.adam(learning_rate) \n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model"
      ],
      "metadata": {
        "id": "FMRDDW0JUCL3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**defining loss function**"
      ],
      "metadata": {
        "id": "z1SX4YdxUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#serial\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2) \n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  metrics = {\n",
        "      'loss': loss,     #LOSS\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "iYslN8Z7UCL4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train step defination**"
      ],
      "metadata": {
        "id": "aRW6GB7RUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cpu serial\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    image, label = batch  \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);\n",
        "        loss =  image_difference_loss(logits, label);\n",
        "        return loss, logits\n",
        "\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "    (_, logits), grads = gradient_fn(state.params)\n",
        "    new_state = state.apply_gradients(grads=grads)\n",
        "    logs = compute_metrics(logits=logits, labels=label)\n",
        "    return new_state, logs\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, image):\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=image)\n"
      ],
      "metadata": {
        "id": "KUYaqngCUCL4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image conversion fiunctions**"
      ],
      "metadata": {
        "id": "Kz85hOhsUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu"
      ],
      "metadata": {
        "id": "qb1oYFIXUCL4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**image dataset, image size and batch size Setup**"
      ],
      "metadata": {
        "id": "3BSv3BxMUCL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########################################## to load the data in batches as mentioned single batch of images with already provided sizes \n",
        "import jax\n",
        "from jax import random\n",
        "import jax.numpy as jnp\n",
        "\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "batch_size = 40\n",
        "\n",
        "import os\n",
        "x_image_dir = r'/content/MEDIAPIPEinput/'\n",
        "y_image_dir = r'/content/annotated_images/'\n",
        "#############################################################################\n",
        "bandend = [\"c\",\".png\", \"b02\"]\n",
        "expression_b2 = bandend[1]\n",
        "x_total_images =  [f for f in os.listdir(x_image_dir) if f.__contains__(expression_b2)]\n",
        "x_total_images.sort()\n",
        "x_total_images_path = [os.path.join(x_image_dir, i) for i in x_total_images if i != 'outputs']\n",
        "no_of_batches = int(len(x_total_images_path)/batch_size)\n",
        "\n",
        "\n",
        "y_total_images =  [f for f in os.listdir(y_image_dir) if f.__contains__(expression_b2)]\n",
        "y_total_images.sort()\n",
        "y_total_images_path = [os.path.join(y_image_dir, i) for i in y_total_images if i != 'outputs']\n",
        "######################################## making 8 array of input for each device >>>\n",
        "def batchedimages(total_images_path, image_locations):\n",
        "  RGB8 = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[0]))\n",
        "  return RGB8\n",
        "\n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(x_total_images_path))\n",
        "  x_img_list = []\n",
        "  y_img_list = []\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    x_img_list.append(batchedimages(x_total_images_path, batch_idx))\n",
        "    y_img_list.append(batchedimages(y_total_images_path, batch_idx))\n",
        "  yield jnp.array(x_img_list), jnp.array(y_img_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "tveEVVN5UCL4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "working here ðŸ”»"
      ],
      "metadata": {
        "id": "9ng0ow4t2JN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = data_stream()  \n",
        "next(batches)[0].shape"
      ],
      "metadata": {
        "id": "rIpc4pweDxRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as onp\n",
        "onp.array(jnp.array(x_img_list))\n"
      ],
      "metadata": {
        "id": "lQ0Hi47h298P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **ðŸ‘ HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from google.colab import output\n",
        "import orbax.checkpoint as orbax\n",
        "from flax.training import checkpoints\n",
        "\n",
        "import optax\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "CKPT_DIR = 'ckpts'\n",
        "\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  batches = data_stream()\n",
        "  BATCH, H, W, Channels = next(batches)[0].shape\n",
        "  state = Create_train_state( rng, model, (BATCH, H, W, Channels ), learning_rate ) \n",
        "  count = 1\n",
        "#âœ…âœ…ðŸ”» state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "\n",
        "###################### \n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "\n",
        "total_epochs = 50\n",
        "for epochs in tqdm(range(no_of_batches-5)):  \n",
        "  batches = data_stream()\n",
        "\n",
        "  if checkpoint_available:\n",
        "    state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r {dir}\n",
        "  input_data = next(batches)\n",
        "  for bbb in tqdm(range(total_epochs)):\n",
        "    state, metrics = train_step(state, input_data, rng)\n",
        "    # output.clear()\n",
        "    print(\"loss: \",metrics['loss'],\" <<< \") # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  orbax_checkpointer = orbax.Checkpointer(orbax.PyTreeCheckpointHandler())\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step=epochs, prefix='checkpoint_', keep=1, overwrite=False, orbax_checkpointer=orbax_checkpointer)\n",
        "  # restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rs2HKO3NUCL5",
        "outputId": "1bd26406-7b3c-41ef-bf5d-83314cbacc7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 140, 140, 3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|â–         | 1/50 [00:13<11:18, 13.85s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  28754.871  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  4%|â–         | 2/50 [00:14<05:02,  6.30s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  28537.637  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  6%|â–Œ         | 3/50 [00:15<03:01,  3.87s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  28328.125  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  8%|â–Š         | 4/50 [00:16<02:05,  2.72s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  28122.154  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 10%|â–ˆ         | 5/50 [00:17<01:33,  2.09s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  27919.215  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 12%|â–ˆâ–        | 6/50 [00:18<01:14,  1.70s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  27715.63  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 14%|â–ˆâ–        | 7/50 [00:19<01:02,  1.46s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  27506.05  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 16%|â–ˆâ–Œ        | 8/50 [00:20<00:54,  1.31s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  27288.76  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 18%|â–ˆâ–Š        | 9/50 [00:21<00:49,  1.20s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  27062.285  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 20%|â–ˆâ–ˆ        | 10/50 [00:22<00:45,  1.13s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  26827.566  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 22%|â–ˆâ–ˆâ–       | 11/50 [00:23<00:41,  1.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  26585.457  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 24%|â–ˆâ–ˆâ–       | 12/50 [00:24<00:39,  1.03s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  26336.986  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 26%|â–ˆâ–ˆâ–Œ       | 13/50 [00:25<00:37,  1.01s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  26079.576  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 28%|â–ˆâ–ˆâ–Š       | 14/50 [00:26<00:36,  1.00s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  25807.354  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [00:27<00:34,  1.00it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  25522.826  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [00:28<00:33,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  25220.564  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [00:29<00:32,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  24896.145  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [00:30<00:31,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  24546.928  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [00:31<00:30,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  24172.424  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [00:32<00:29,  1.00it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  23771.852  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [00:33<00:28,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  23342.11  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [00:34<00:27,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  22878.37  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [00:35<00:29,  1.10s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  22372.623  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [00:36<00:27,  1.07s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  21823.19  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [00:37<00:25,  1.04s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  21235.846  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [00:38<00:24,  1.02s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  20607.326  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [00:39<00:23,  1.00s/it]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  19936.164  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [00:40<00:21,  1.00it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  19220.477  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [00:41<00:20,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  18459.773  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [00:42<00:19,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  17655.35  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [00:43<00:18,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  16814.012  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [00:44<00:17,  1.01it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  15943.295  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [00:45<00:16,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  15046.98  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [00:46<00:15,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  14145.505  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [00:47<00:14,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  13263.837  <<< \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [00:48<00:13,  1.02it/s]\u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  12431.478  <<< \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "HuKiT35-TlKD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inference engine**"
      ],
      "metadata": {
        "id": "yZwxFsKYUCL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# newsize = (140,140) #(260, 260) # /.... 233 * 454\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np \n",
        "from google.colab import output\n",
        "\n",
        "!wget https://live.staticflickr.com/7492/15677707699_d9d67acf9d_b.jpg -O a.jpg\n",
        "image_in = '/content/a.jpg'\n",
        "\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "image = jnp.asarray((imageRGB(image_in)[1]))\n",
        "#restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "#state = restored_state\n",
        "prediction = eval_step(state, image)\n",
        "prediction['loss']\n",
        "\n",
        "\n",
        "predicted_image = np.array(prediction['logits'],  dtype=np.uint8).reshape(newsize) \n",
        "cv2_imshow(predicted_image)\n"
      ],
      "metadata": {
        "id": "u4qN8DkhUCL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# from google.colab.patches import cv2_imshow\n",
        "# import numpy as np \n",
        "# def show_image(argu):\n",
        "#   L1 = argu[0]\n",
        "#   predicted_image = np.array(argu[0],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "#   a = predicted_image\n",
        "#   for i in range(0,argu.shape[0]):\n",
        "#     predicted_image = np.array(argu[i],  dtype=np.uint8).reshape(newsize) \n",
        "#     a = cv2.hconcat([a, predicted_image])\n",
        "#   cv2_imshow(a)\n",
        "\n",
        "# show_image(metrics['logits'])"
      ],
      "metadata": {
        "id": "tMXLUB6hUCL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from jax.tree_util import tree_structure\n",
        "# print(tree_structure(state))"
      ],
      "metadata": {
        "id": "BN0qnfr0UCL5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}