{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/Media-Segment-Depth-MLP/blob/main/Segmentation_Depth_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***RUNNING MEDIAPIPE HOLISTIC***"
      ],
      "metadata": {
        "id": "-juv_T5TX5Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/ --output-document=test.mp4 https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/video.mp4 "
      ],
      "metadata": {
        "id": "K-EszVm8pR_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install mediapipe"
      ],
      "metadata": {
        "id": "BUW-yGjEYGey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_location = '/content/test.mp4'\n",
        "import os\n",
        "\n",
        " \n",
        "# Read images with OpenCV.\n",
        "#images= None\n",
        "image_dir = '/content/MEDIAPIPEinput/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "image_dir_out = '/content/annotated_images'\n",
        "os.makedirs(image_dir_out, exist_ok=True)\n",
        "frame_rate = 24\n",
        "!ffmpeg -y \\\n",
        "  -i {video_location} \\\n",
        "  -r {frame_rate} {image_dir}out_%09d.png\n",
        "\n",
        "imgs_list = os.listdir(image_dir)\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n"
      ],
      "metadata": {
        "id": "XXwQvA_2bMmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "# mp_holistic = mp.solutions.holistic\n",
        "mp_pose = mp.solutions.pose\n",
        "!rm -r {image_dir}.ipynb_checkpoints\n",
        "\n",
        "# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.\n",
        "with mp_pose.Pose(static_image_mode=True, \n",
        "                          min_detection_confidence=0.2,\n",
        "                          model_complexity=2, \n",
        "                          enable_segmentation=True,) as pose:\n",
        "  temp_segmentation_mask =[]                        \n",
        "  for name, image in enumerate(imgs_path):\n",
        "    !rm -r {image_dir}.ipynb_checkpoints\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "    image = cv2.imread(image)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # Draw pose segmentation.\n",
        "    print(f'Pose segmentation of {name}:')\n",
        "    annotated_image_pose = image.copy()\n",
        "    red_img = np.zeros_like(annotated_image_pose, dtype=np.uint8)\n",
        "    red_img[:, :] = (255,255,255)\n",
        "    ###check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    if results.segmentation_mask is None:\n",
        "      print(\"true\")\n",
        "      results.segmentation_mask = temp_segmentation_mask[-1]\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    else:\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    ###End check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    segm_2class = 0.0 + 1.0 * results.segmentation_mask\n",
        "    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "    annotated_image_pose = annotated_image_pose * segm_2class + red_img * (1 - segm_2class)\n",
        "    #resize_and_show(annotated_image)\n",
        "    cv2.imwrite('%s/%s' %(image_dir_out, imgs_list[name]), annotated_image_pose)\n",
        "    !rm -r {image_dir_out}.ipynb_checkpoints\n",
        "\n",
        "# # Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.\n",
        "# with mp_holistic.Holistic(static_image_mode=False, \n",
        "#                           min_detection_confidence=0.5,\n",
        "#                           model_complexity=2, \n",
        "#                           enable_segmentation=True,) as holistic:\n",
        "#   temp_segmentation_mask =[]                        \n",
        "#   for name, image in enumerate(imgs_path):\n",
        "#     !rm -r {image_dir}.ipynb_checkpoints\n",
        "#     # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "#     image = cv2.imread(image)\n",
        "#     results = holistic.process(image)\n",
        "\n",
        "#     # Draw pose segmentation.\n",
        "#     print(f'Pose segmentation of {name}:')\n",
        "#     annotated_image_pose = image.copy()\n",
        "#     red_img = np.zeros_like(annotated_image_pose, dtype=np.uint8)\n",
        "#     red_img[:, :] = (255,255,255)\n",
        "#     if results.segmentation_mask is None:\n",
        "#       print(\"true\")\n",
        "#       results.segmentation_mask = temp_segmentation_mask[-1]\n",
        "#       temp_segmentation_mask.append(results.segmentation_mask)\n",
        "#     else:\n",
        "#       temp_segmentation_mask.append(results.segmentation_mask)\n",
        "#     segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
        "#     segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "#     annotated_image_pose = annotated_image_pose * segm_2class + red_img * (1 - segm_2class)\n",
        "#     #resize_and_show(annotated_image)\n",
        "#     cv2.imwrite('%s/%s' %(image_dir_out, imgs_list[name]), annotated_image_pose)\n",
        "#     !rm -r {image_dir_out}.ipynb_checkpoints"
      ],
      "metadata": {
        "id": "SNuLCc3IYPsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_rate = 24\n",
        "#create video 1\n",
        "!ffmpeg \\\n",
        "  -framerate {frame_rate} \\\n",
        "  -pattern_type glob \\\n",
        "  -i {image_dir}'/*.png' \\\n",
        "  inputA.mp4\n",
        "#create vodeo 2\n",
        "!ffmpeg \\\n",
        "  -framerate {frame_rate} \\\n",
        "  -pattern_type glob \\\n",
        "  -i {image_dir_out}'/*.png' \\\n",
        "  inputB.mp4\n",
        "#merge video 1 and video 2\n",
        "!ffmpeg \\\n",
        "  -i inputA.mp4 -i inputB.mp4 \\\n",
        "  -filter_complex \\\n",
        "  \"[0:v][1:v]vstack=inputs=2\" \\\n",
        "  finalOutput.mp4\n",
        "#add sound of main video to final output video\n",
        "!ffmpeg \\\n",
        "  -i 'finalOutput.mp4' -i {video_location} \\\n",
        "  -c:v copy -c:a copy -map 0:v:0 -map 1:a:0 \\\n",
        "  OUTPUT_FILE.mp4"
      ],
      "metadata": {
        "id": "-0ag81xGoP3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***BoostingMonocularDepth code***"
      ],
      "metadata": {
        "id": "1G67vqC1-O3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 1\n",
        "INPUT_FILES = 'input1'\n",
        "OUTPUT_FILES = 'outputs_leres'\n",
        "!mkdir -p {INPUT_FILES}\n",
        "!mkdir -p {OUTPUT_FILES}"
      ],
      "metadata": {
        "id": "HrdbNfdQcIDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cUXERYUmqsyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -hwaccel cuvid \\\n",
        "  -i '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/test.mp4' \\\n",
        "  -r 24 {INPUT_FILES}/out_%09d.png"
      ],
      "metadata": {
        "id": "ZBJJQKD4azSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 2\n",
        "\n",
        "\n",
        "# Clone git repo\n",
        "!git clone https://github.com/1kaiser/BoostingMonocularDepth.git\n",
        "!mkdir -p BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/latest_net_G.pth\" \"BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\"\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/res101.pth\" \"BoostingMonocularDepth/\"\n"
      ],
      "metadata": {
        "id": "cOCuEvGmkvBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code section 2\n",
        "\n",
        "\n",
        "# # Clone git repo\n",
        "# !git clone https://github.com/1kaiser/BoostingMonocularDepth.git\n",
        "\n",
        "# # Downloading merge model weights \n",
        "# !mkdir -p /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "# !gdown 1BbX2_NjrmrzRIB2CRJ-1rNRAjPABXPCh\n",
        "# !mv latest_net_G.pth /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "\n",
        "# # Downloading Midas weights\n",
        "# #!gdown https://drive.google.com/uc?id=1nqW_Hwj86kslfsXR7EnXpEWdO2csz1cC\n",
        "# #!mv model.pt /content/BoostingMonocularDepth/midas/\n",
        "\n",
        "# # # Downloading LeRes weights \n",
        "# !gdown 1vni3Oh4y0S4eatz_u_aDnwDLwxBjUpCh\n",
        "# !mv res101.pth /content/BoostingMonocularDepth/"
      ],
      "metadata": {
        "id": "MHjb_2jxbHjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 3\n",
        "%cd BoostingMonocularDepth/"
      ],
      "metadata": {
        "id": "rAOyd3jnbfRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check and delete  fiiles"
      ],
      "metadata": {
        "id": "QCNFJkXaR-ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FILES = '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive/input1'\n",
        "OUTPUT_FILES = \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive/outputs_leres\""
      ],
      "metadata": {
        "id": "aF3FVhNbSPNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile goDeleteHolistic.js\n",
        "'use strict';\n",
        "\n",
        "const fs = require('fs');\n",
        "let d=0;\n",
        "let darray1 = fs.readdirSync({INPUT_FILES});\n",
        "let darray2 = fs.readdirSync({OUTPUT_FILES});\n",
        "for (let index = 0; index < darray1.length; index++) {\n",
        "    console.log(index)\n",
        "    if (darray2.includes(darray1[index])) {\n",
        "        console.log(d++)\n",
        "        \n",
        "        fs.unlinkSync({INPUT_FILES}+darray1[index])\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "xoDYwaPER849"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node goDeleteHolistic.js"
      ],
      "metadata": {
        "id": "tFtDRDGcSDMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the method using LeRes\n",
        "!python run.py --Final --data_dir {INPUT_FILES} --output_dir  {OUTPUT_FILES}/ --depthNet 2"
      ],
      "metadata": {
        "id": "gDgQ4X_ubiNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Video Output**"
      ],
      "metadata": {
        "id": "ZIBs_0aDbq46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create video 1\n",
        "!ffmpeg \\\n",
        "-framerate 24 \\\n",
        "-pattern_type glob \\\n",
        "-i {INPUT_FILES}'/*.png' \\\n",
        "input0.mp4\n",
        "#create vodeo 2\n",
        "!ffmpeg \\\n",
        "-framerate 24 \\\n",
        "-pattern_type glob \\\n",
        "-i {OUTPUT_FILES}'/*.png' \\\n",
        "input1.mp4\n",
        "#merge video 1 and video 2\n",
        "!ffmpeg \\\n",
        "-i input0.mp4 -i input1.mp4 \\\n",
        "-filter_complex \\\n",
        "\"[0:v][1:v]vstack=inputs=2\" \\\n",
        "finalOutput.mp4\n",
        "#add sound of main video to final output video\n",
        "!ffmpeg \\\n",
        "-i 'finalOutput.mp4' -i 'm.mp4' \\\n",
        "-c:v copy -c:a copy -map 0:v:0 -map 1:a:0 \\\n",
        "OUTPUT_FILE.mp4"
      ],
      "metadata": {
        "id": "IHv3_wz1bs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzkIeHiNc9Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pd0LfKcGizVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive"
      ],
      "metadata": {
        "id": "rPX-Npz7qQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "EF4kDZevqkmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FFMPEG video to image**"
      ],
      "metadata": {
        "id": "BpmpJgkHa0fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MLP MODEL WITH POSITIONAL ENCODER**"
      ],
      "metadata": {
        "id": "UFAs21IJafFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icmWdrOM9Rx1"
      },
      "outputs": [],
      "source": [
        "############<< LOADING DATASET >>#############\n",
        "\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/archive.zip\" \"/content/\"\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive..\n",
        "!mkdir -p /content/flowers/all\n",
        "!cp /content/flowers/daisy/* /content/flowers/all\n",
        "!cp /content/flowers/dandelion/* /content/flowers/all\n",
        "!cp /content/flowers/rose/* /content/flowers/all\n",
        "!cp /content/flowers/sunflower/* /content/flowers/all\n",
        "!cp /content/flowers/tulip/* /content/flowers/all\n",
        "\n",
        "##############$$$$$$$$$$$$$$<<< IMPORTING LIBRARIES >>>$$$$$$$$$$$$$$$$################\n",
        "#âœ…\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "!python -m pip install -q -U flax\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "##########################################<< FFT PSEUDO SIN COS POSITIONAL ENCODER >>#########################################\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(inputs):\n",
        "    print(\"positional_encoding start\")\n",
        "    batch_size, _ = inputs.shape;print(inputs.shape)\n",
        "    inputs_freq = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(inputs_freq.shape)\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)]);print(x.shape)\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([batch_size, -1]);print(x.shape)\n",
        "    x = jnp.concatenate([inputs, x], axis=-1);print(x.shape)\n",
        "    print(\"positional_encoding end\")\n",
        "    return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "num_dense_layers = 8 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        print(\"network model start\")\n",
        "        print(x.shape)\n",
        "        for i in range(num_dense_layers):\n",
        "            x = nn.Dense(\n",
        "                dense_layer_width,\n",
        "                dtype=self.dtype,\n",
        "                precision=self.precision\n",
        "                )(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "            print(x.shape)\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        print(x.shape)\n",
        "        print(\"network model end\")\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "     \n",
        "##########################################<< CREATE MODEL INITIAL STATE FILLING WITH WEIGHTS >>#########################################\n",
        "\n",
        "def init_train_state(model, r_key, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    init_variables = model.init(r_key, jnp.ones(shape))  # Initialize the Model\n",
        "    optimizer = optax.adam(learning_rate) # Create the optimizer\n",
        "    # Create a State\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=init_variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model\n",
        "\n",
        "key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "x = jnp.empty((batch_size_no, 28, 28, 1)) # Dummy Input\n",
        "_, image_height, image_width, channels = x.shape\n",
        "##########################################<< CREATING MODEL STATE>>#########################################\n",
        "\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n",
        "state\n",
        "print(\"state initiated âœ…\")\n",
        "\n",
        "##########################################<< MODEL LOSS METRICS FOR GRADIENT COMPUTATION >>#########################################\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2)\n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  loss = lax.pmean(loss, axis_name=\"batch\");print(\"ok4\")\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "##########################################<< MODEL TRAINING STEP FOR UPDATING WEIGHTS FROM CALCULATED GRAGIENTS FROM CHANGING MODEL STATE >>#########################################\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    print(batch)\n",
        "    image, label = batch\n",
        "    print(image,\"<<<image\")\n",
        "    print(label,\"<<<label\")    \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);print(\"done1\",logits.shape)\n",
        "        loss =  image_difference_loss(logits, label);print(\"done2\",loss.shape)\n",
        "        return loss, logits\n",
        "\n",
        "    print(\"ok1really\")\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True);print(\"ok1\")\n",
        "    (_, logits), grads = gradient_fn(state.params);print(\"ok2\")\n",
        "    #train_loss, gradients_each = jax.value_and_grad(loss_fn)(state.params);print(\"ok3\")\n",
        "    grads = lax.pmean(grads,\"batch\");print(\"ok4\")\n",
        "    # grads = jnp.mean(grads);print(\"ok4\")\n",
        "    state = state.apply_gradients(grads=grads);print(\"ok5\")\n",
        "    # train_loss = jnp.mean(train_loss);print(\"ok6\")\n",
        "    logs = compute_metrics(logits=logits, labels=label);print(\"ok7\")\n",
        "    return state, logs\n",
        "\n",
        "parallel_train_step = jax.pmap(train_step, \"batch\")\n",
        "# parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", in_axes = (0, 0, 0))\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    image, label = batch\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=label)\n",
        "\n",
        "##########################################<< DATASET GENERATING FUNCTION HELPER IN GRB TO GRAY CONVERSION >>#########################################\n",
        "newsize = (150, 150) # /.... 233 * 454\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "\n",
        "\n",
        "##########################################<< ANALYSING DATASETS FOR BATCH DISTRIBUTION >>#########################################\n",
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = jax.device_count()    #///// batchsize depending on number of devices available for processing\n",
        "import os\n",
        "image_dir = r'/content/flowers/all/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".jpg\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "\n",
        "\n",
        "\n",
        "##########################################<< RUNNING TRAINNING >>#########################################\n",
        "#@title # **ðŸ‘ HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[1]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[2]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[3]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[4]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[5]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[6]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[7]])[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(total_images_path[image_locations[0]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[1]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[2]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[3]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[4]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[5]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[6]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[7]])[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "  \n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "\n",
        "next(batches)[0].shape ### this command starts initial 8 image  stream, if callled inside a iteration loop then it will get next images for calculations>>>\n",
        "vv, shapea, channels = next(batches)[0].shape # seitting values >>> [0] 8 784 3  [1] 8 784 1 ; RGB & GRAYSCALE versions 8 images each converted to 1-D array\n",
        "print(vv, shapea, channels)\n",
        "######################<<< summary writer for tensor board\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# logdir = \"runs\"\n",
        "# writer = SummaryWriter(logdir)\n",
        "######################\n",
        "######################\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "#################################<<< checking if checkpoint already available\n",
        "import os # importing os module\n",
        "import re # to find file using regular expression\n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "#################################\n",
        "##########################################<<< loading checkpoint by checking the Flag available\n",
        "from flax.training import checkpoints\n",
        "if checkpoint_available:\n",
        "  CKPT_DIR = 'ckpts'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "  #state = flax.jax_utils.replicate(restored_state)\n",
        "  print(\"true <<< File loaded for and replicated to all devices\")\n",
        "##########################################\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "  count = 1\n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "from flax.training import checkpoints\n",
        "import time\n",
        "total_epochs = 50\n",
        "for epochs in range(total_epochs):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "  start_time = time.time()\n",
        "  batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "  if checkpoint_available:\n",
        "    CKPT_DIR = 'ckpts'\n",
        "    restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    state = restored_state\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r /content/ckpts\n",
        "  for bbb in range(no_of_batches-5):\n",
        "    print(bbb,\"of total number of batches\",no_of_batches)\n",
        "    state, metrics = parallel_train_step(state, next(batches), dropout_rngs)\n",
        "    print(\"<<âœ…âœ…âœ…epoc : \",epochs,\" completeâœ…âœ…âœ…>>\\n\",metrics['loss'][0]) #printing loss 1 out of 8 processed in 8 devices\n",
        "    \n",
        "    #############################################<<< output visualization ðŸ‘ðŸ»\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    import numpy as np        # source : https://www.folkstalk.com/tech/how-to-convert-numpy-array-to-cv2-image-with-code-examples/\n",
        "    \n",
        "    print(\"logits shape âš¡âš¡âš¡âš¡\", metrics['logits'][0].shape)\n",
        "    L1 = metrics['logits'][0]\n",
        "    # predicted_image = np.array(L1,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "    # cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "    for i in range(0,metrics['labels'].shape[0]):\n",
        "      print(i)\n",
        "      predicted_image = np.array(metrics['logits'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "      cv2_imshow(predicted_image)\n",
        "\n",
        "      print(\"labels shape âš¡âš¡âš¡âš¡\", metrics['labels'][0].shape)\n",
        "      L2 = metrics['labels'][0]\n",
        "      actual_image = np.array(metrics['labels'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array newsize = image_width , image_height\n",
        "      cv2img = cv2_imshow(actual_image) # This work the same as passing an image\n",
        "    #############################################<<< output visualization ðŸ‘ðŸ»\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    writer.add_scalar('Loss', int(metrics['loss'][0]), epochs)\n",
        "  epoch_time = time.time() - start_time\n",
        "  print(f\"Epoch {epochs} in {epoch_time:0.2f} sec\")\n",
        "  ##################################################<<< model saving mechanism for flax model state as checkpoints for each epochs,\"checkpoint\" is a terminology that means all the model weights and biases during the calculation till the completion of 1 epoch were being updated, then this final set of weights and biases including their placement inside the model will be saves as a (schema+weight values) saved as checkpoint in the mentioned <<CKPT_DIR = 'ckpts'>> mentioned folder.  \n",
        "  CKPT_DIR = 'ckpts'\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step= epochs)     # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "  # images = total_images_path[batch_idx]\n",
        "writer.flush()\n",
        "\n",
        " "
      ]
    }
  ]
}