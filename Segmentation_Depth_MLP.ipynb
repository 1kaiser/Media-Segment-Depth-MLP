{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1kaiser/Media-Segment-Depth-MLP/blob/main/Segmentation_Depth_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***RUNNING MEDIAPIPE HOLISTIC***"
      ],
      "metadata": {
        "id": "-juv_T5TX5Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /content/ --output-document=test.mp4 https://github.com/1kaiser/Media-Segment-Depth-MLP/releases/download/v0.2/video.mp4 "
      ],
      "metadata": {
        "id": "K-EszVm8pR_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "P7mqYLvH4Elq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install mediapipe"
      ],
      "metadata": {
        "id": "BUW-yGjEYGey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_location = '/content/test.mp4'\n",
        "import os\n",
        "\n",
        " \n",
        "# Read images with OpenCV.\n",
        "#images= None\n",
        "image_dir = '/content/MEDIAPIPEinput/'\n",
        "os.makedirs(image_dir, exist_ok=True)\n",
        "image_dir_out = '/content/annotated_images'\n",
        "os.makedirs(image_dir_out, exist_ok=True)\n",
        "frame_rate = 24\n",
        "!ffmpeg -y \\\n",
        "  -i {video_location} \\\n",
        "  -r {frame_rate} {image_dir}out_%09d.png\n",
        "\n",
        "imgs_list = os.listdir(image_dir)\n",
        "imgs_list.sort()\n",
        "imgs_path = [os.path.join(image_dir, i) for i in imgs_list if i != 'outputs']\n"
      ],
      "metadata": {
        "id": "XXwQvA_2bMmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "# mp_holistic = mp.solutions.holistic\n",
        "mp_pose = mp.solutions.pose\n",
        "!rm -r {image_dir}.ipynb_checkpoints\n",
        "\n",
        "# Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.\n",
        "with mp_pose.Pose(static_image_mode=True, \n",
        "                          min_detection_confidence=0.2,\n",
        "                          model_complexity=2, \n",
        "                          enable_segmentation=True,) as pose:\n",
        "  temp_segmentation_mask =[]                        \n",
        "  for name, image in enumerate(imgs_path):\n",
        "    !rm -r {image_dir}.ipynb_checkpoints\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "    image = cv2.imread(image)\n",
        "    results = pose.process(image)\n",
        "\n",
        "    # Draw pose segmentation.\n",
        "    print(f'Pose segmentation of {name}:')\n",
        "    annotated_image_pose = image.copy()\n",
        "    red_img = np.zeros_like(annotated_image_pose, dtype=np.uint8)\n",
        "    red_img[:, :] = (255,255,255)\n",
        "    ###check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    if results.segmentation_mask is None:\n",
        "      print(\"true\")\n",
        "      results.segmentation_mask = temp_segmentation_mask[-1]\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    else:\n",
        "      temp_segmentation_mask.append(results.segmentation_mask)\n",
        "    ###End check if segmentation_mask exists or not ## if exists then ok Else use previous mask temporarily\n",
        "    segm_2class = 0.0 + 1.0 * results.segmentation_mask\n",
        "    segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "    annotated_image_pose = annotated_image_pose * segm_2class + red_img * (1 - segm_2class)\n",
        "    #resize_and_show(annotated_image)\n",
        "    cv2.imwrite('%s/%s' %(image_dir_out, imgs_list[name]), annotated_image_pose)\n",
        "    !rm -r {image_dir_out}.ipynb_checkpoints\n",
        "\n",
        "# # Run MediaPipe Pose with `enable_segmentation=True` to get pose segmentation.\n",
        "# with mp_holistic.Holistic(static_image_mode=False, \n",
        "#                           min_detection_confidence=0.5,\n",
        "#                           model_complexity=2, \n",
        "#                           enable_segmentation=True,) as holistic:\n",
        "#   temp_segmentation_mask =[]                        \n",
        "#   for name, image in enumerate(imgs_path):\n",
        "#     !rm -r {image_dir}.ipynb_checkpoints\n",
        "#     # Convert the BGR image to RGB and process it with MediaPipe Pose.\n",
        "#     image = cv2.imread(image)\n",
        "#     results = holistic.process(image)\n",
        "\n",
        "#     # Draw pose segmentation.\n",
        "#     print(f'Pose segmentation of {name}:')\n",
        "#     annotated_image_pose = image.copy()\n",
        "#     red_img = np.zeros_like(annotated_image_pose, dtype=np.uint8)\n",
        "#     red_img[:, :] = (255,255,255)\n",
        "#     if results.segmentation_mask is None:\n",
        "#       print(\"true\")\n",
        "#       results.segmentation_mask = temp_segmentation_mask[-1]\n",
        "#       temp_segmentation_mask.append(results.segmentation_mask)\n",
        "#     else:\n",
        "#       temp_segmentation_mask.append(results.segmentation_mask)\n",
        "#     segm_2class = 0.2 + 0.8 * results.segmentation_mask\n",
        "#     segm_2class = np.repeat(segm_2class[..., np.newaxis], 3, axis=2)\n",
        "#     annotated_image_pose = annotated_image_pose * segm_2class + red_img * (1 - segm_2class)\n",
        "#     #resize_and_show(annotated_image)\n",
        "#     cv2.imwrite('%s/%s' %(image_dir_out, imgs_list[name]), annotated_image_pose)\n",
        "#     !rm -r {image_dir_out}.ipynb_checkpoints"
      ],
      "metadata": {
        "id": "SNuLCc3IYPsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_rate = 24\n",
        "#create video 1\n",
        "!ffmpeg \\\n",
        "  -framerate {frame_rate} \\\n",
        "  -pattern_type glob \\\n",
        "  -i {image_dir}'/*.png' \\\n",
        "  inputA.mp4\n",
        "#create vodeo 2\n",
        "!ffmpeg \\\n",
        "  -framerate {frame_rate} \\\n",
        "  -pattern_type glob \\\n",
        "  -i {image_dir_out}'/*.png' \\\n",
        "  inputB.mp4\n",
        "#merge video 1 and video 2\n",
        "!ffmpeg \\\n",
        "  -i inputA.mp4 -i inputB.mp4 \\\n",
        "  -filter_complex \\\n",
        "  \"[0:v][1:v]vstack=inputs=2\" \\\n",
        "  finalOutput.mp4\n",
        "#add sound of main video to final output video\n",
        "!ffmpeg \\\n",
        "  -i 'finalOutput.mp4' -i {video_location} \\\n",
        "  -c:v copy -c:a copy -map 0:v:0 -map 1:a:0 \\\n",
        "  OUTPUT_FILE.mp4"
      ],
      "metadata": {
        "id": "-0ag81xGoP3_",
        "outputId": "0054e663-ab24-4682-e0e8-41ca0eaf34b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, image2, from '/content/MEDIAPIPEinput//*.png':\n",
            "  Duration: 00:00:40.63, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 1050x440, 24 fps, 24 tbr, 24 tbn, 24 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mprofile High 4:4:4 Predictive, level 3.1, 4:4:4 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=14 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'inputA.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv444p, 1050x440, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  975 fps=176 q=-1.0 Lsize=    5633kB time=00:00:40.50 bitrate=1139.3kbits/s speed= 7.3x    \n",
            "video:5621kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.211756%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mframe I:7     Avg QP:20.42  size: 41331\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mframe P:272   Avg QP:22.74  size: 12877\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mframe B:696   Avg QP:26.28  size:  2821\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mconsecutive B-frames:  1.3%  4.9% 16.6% 77.1%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mmb I  I16..4: 24.5%  0.0% 75.5%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mmb P  I16..4:  5.3%  0.0%  9.8%  P16..4: 40.1% 15.7%  6.9%  0.0%  0.0%    skip:22.3%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mmb B  I16..4:  0.3%  0.0%  0.7%  B16..8: 37.6%  5.3%  1.3%  direct: 1.1%  skip:53.8%  L0:44.7% L1:50.7% BI: 4.5%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mcoded y,u,v intra: 61.8% 24.2% 26.1% inter: 8.2% 1.7% 1.8%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mi16 v,h,dc,p: 43% 22% 19% 17%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 15% 15%  6%  8% 10%  7%  7%  5%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mWeighted P-Frames: Y:0.4% UV:0.4%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mref P L0: 59.6% 17.6% 16.9%  5.9%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mref B L0: 91.6%  7.1%  1.3%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mref B L1: 98.4%  1.6%\n",
            "\u001b[1;36m[libx264 @ 0x55b11f138ec0] \u001b[0mkb/s:1133.29\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, image2, from '/content/annotated_images/*.png':\n",
            "  Duration: 00:00:40.63, start: 0.000000, bitrate: N/A\n",
            "    Stream #0:0: Video: png, rgb24(pc), 1050x440, 24 fps, 24 tbr, 24 tbn, 24 tbc\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (png (native) -> h264 (libx264))\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mprofile High 4:4:4 Predictive, level 3.1, 4:4:4 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=14 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'inputB.mp4':\n",
            "  Metadata:\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv444p, 1050x440, q=-1--1, 24 fps, 12288 tbn, 24 tbc\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  975 fps=304 q=-1.0 Lsize=    2054kB time=00:00:40.50 bitrate= 415.5kbits/s speed=12.6x    \n",
            "video:2043kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.573156%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mframe I:63    Avg QP: 8.85  size:  4840\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mframe P:283   Avg QP:21.52  size:  3110\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mframe B:629   Avg QP:26.78  size:  1440\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mconsecutive B-frames: 10.7%  6.2% 11.4% 71.8%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mmb I  I16..4: 89.8%  0.0% 10.2%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mmb P  I16..4:  3.3%  0.0%  4.6%  P16..4:  6.6%  3.1%  1.0%  0.0%  0.0%    skip:81.4%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mmb B  I16..4:  0.4%  0.0%  1.1%  B16..8:  9.7%  2.6%  0.5%  direct: 0.6%  skip:85.1%  L0:47.4% L1:47.7% BI: 4.9%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mcoded y,u,v intra: 22.3% 4.8% 6.0% inter: 2.7% 0.1% 0.2%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mi16 v,h,dc,p: 90%  2%  4%  3%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 28% 13% 16%  6%  9% 10%  7%  7%  4%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mWeighted P-Frames: Y:1.4% UV:1.4%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mref P L0: 58.1% 12.4% 20.2%  9.2%  0.0%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mref B L0: 81.4% 14.7%  3.9%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mref B L1: 93.8%  6.2%\n",
            "\u001b[1;36m[libx264 @ 0x55b4523ce400] \u001b[0mkb/s:411.74\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'inputA.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:40.63, start: 0.000000, bitrate: 1135 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1050x440, 1133 kb/s, 24 fps, 24 tbr, 12288 tbn, 48 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from 'inputB.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:40.63, start: 0.000000, bitrate: 414 kb/s\n",
            "    Stream #1:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1050x440, 411 kb/s, 24 fps, 24 tbr, 12288 tbn, 48 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Stream mapping:\n",
            "  Stream #0:0 (h264) -> vstack:input0\n",
            "  Stream #1:0 (h264) -> vstack:input1\n",
            "  vstack -> Stream #0:0 (libx264)\n",
            "Press [q] to stop, [?] for help\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mprofile High 4:4:4 Predictive, level 3.2, 4:4:4 8-bit\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0m264 - core 155 r2917 0a84d98 - H.264/MPEG-4 AVC codec - Copyleft 2003-2018 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=4 threads=24 lookahead_threads=4 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
            "Output #0, mp4, to 'finalOutput.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv444p, 1050x880, q=-1--1, 24 fps, 12288 tbn, 24 tbc (default)\n",
            "    Metadata:\n",
            "      encoder         : Lavc58.54.100 libx264\n",
            "    Side data:\n",
            "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
            "frame=  975 fps=140 q=-1.0 Lsize=    7637kB time=00:00:40.50 bitrate=1544.7kbits/s speed=5.82x    \n",
            "video:7624kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.158873%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mframe I:7     Avg QP:16.07  size: 50525\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mframe P:270   Avg QP:20.89  size: 16413\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mframe B:698   Avg QP:28.50  size:  4329\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mconsecutive B-frames:  1.3%  6.4%  9.8% 82.5%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mmb I  I16..4: 49.9%  0.0% 50.1%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mmb P  I16..4:  5.3%  0.0%  7.8%  P16..4: 23.7% 11.2%  4.4%  0.0%  0.0%    skip:47.6%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mmb B  I16..4:  0.3%  0.0%  1.2%  B16..8: 23.8%  4.4%  1.0%  direct: 0.7%  skip:68.5%  L0:47.2% L1:48.3% BI: 4.5%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mcoded y,u,v intra: 52.4% 16.4% 18.6% inter: 5.4% 0.8% 0.9%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mi16 v,h,dc,p: 66% 14%  9% 12%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 16% 16%  6%  8% 10%  6%  7%  5%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mWeighted P-Frames: Y:0.0% UV:0.0%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mref P L0: 59.8% 16.2% 18.2%  5.8%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mref B L0: 88.4%  9.7%  2.0%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mref B L1: 97.4%  2.6%\n",
            "\u001b[1;36m[libx264 @ 0x5595403cc180] \u001b[0mkb/s:1537.34\n",
            "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
            "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 31.100 / 56. 31.100\n",
            "  libavcodec     58. 54.100 / 58. 54.100\n",
            "  libavformat    58. 29.100 / 58. 29.100\n",
            "  libavdevice    58.  8.100 / 58.  8.100\n",
            "  libavfilter     7. 57.100 /  7. 57.100\n",
            "  libavresample   4.  0.  0 /  4.  0.  0\n",
            "  libswscale      5.  5.100 /  5.  5.100\n",
            "  libswresample   3.  5.100 /  3.  5.100\n",
            "  libpostproc    55.  5.100 / 55.  5.100\n",
            "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'finalOutput.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "  Duration: 00:00:40.63, start: 0.000000, bitrate: 1539 kb/s\n",
            "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1050x880, 1537 kb/s, 24 fps, 24 tbr, 12288 tbn, 48 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "Input #1, mov,mp4,m4a,3gp,3g2,mj2, from '/content/test.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : mp42\n",
            "    minor_version   : 0\n",
            "    compatible_brands: mp42mp41iso4\n",
            "    creation_time   : 2021-12-09T21:02:50.000000Z\n",
            "  Duration: 00:00:40.58, start: 0.000000, bitrate: 673 kb/s\n",
            "    Stream #1:0(und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709), 1050x440, 601 kb/s, 25 fps, 25 tbr, 12800 tbn, 25600 tbc (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-12-09T21:02:50.000000Z\n",
            "      handler_name    : Vireo Eyes v2.7.3\n",
            "      encoder         : AVC Coding\n",
            "    Stream #1:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 65 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-12-09T21:02:50.000000Z\n",
            "      handler_name    : Vireo Ears v2.7.3\n",
            "Output #0, mp4, to 'OUTPUT_FILE.mp4':\n",
            "  Metadata:\n",
            "    major_brand     : isom\n",
            "    minor_version   : 512\n",
            "    compatible_brands: isomiso2avc1mp41\n",
            "    encoder         : Lavf58.29.100\n",
            "    Stream #0:0(und): Video: h264 (High 4:4:4 Predictive) (avc1 / 0x31637661), yuv444p, 1050x880, q=2-31, 1537 kb/s, 24 fps, 24 tbr, 12288 tbn, 12288 tbc (default)\n",
            "    Metadata:\n",
            "      handler_name    : VideoHandler\n",
            "    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 65 kb/s (default)\n",
            "    Metadata:\n",
            "      creation_time   : 2021-12-09T21:02:50.000000Z\n",
            "      handler_name    : Vireo Ears v2.7.3\n",
            "Stream mapping:\n",
            "  Stream #0:0 -> #0:0 (copy)\n",
            "  Stream #1:1 -> #0:1 (copy)\n",
            "Press [q] to stop, [?] for help\n",
            "frame=  975 fps=0.0 q=-1.0 Lsize=    7979kB time=00:00:40.55 bitrate=1611.7kbits/s speed=1.86e+03x    \n",
            "video:7624kB audio:325kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.360190%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***BoostingMonocularDepth code***"
      ],
      "metadata": {
        "id": "1G67vqC1-O3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 1\n",
        "INPUT_FILES = 'input1'\n",
        "OUTPUT_FILES = 'outputs_leres'\n",
        "!mkdir -p {INPUT_FILES}\n",
        "!mkdir -p {OUTPUT_FILES}"
      ],
      "metadata": {
        "id": "HrdbNfdQcIDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "cUXERYUmqsyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ffmpeg -y -hwaccel cuvid \\\n",
        "  -i '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/test.mp4' \\\n",
        "  -r 24 {INPUT_FILES}/out_%09d.png"
      ],
      "metadata": {
        "id": "ZBJJQKD4azSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 2\n",
        "\n",
        "\n",
        "# Clone git repo\n",
        "!git clone https://github.com/1kaiser/BoostingMonocularDepth.git\n",
        "!mkdir -p BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/latest_net_G.pth\" \"BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\"\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/res101.pth\" \"BoostingMonocularDepth/\"\n"
      ],
      "metadata": {
        "id": "cOCuEvGmkvBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Code section 2\n",
        "\n",
        "\n",
        "# # Clone git repo\n",
        "# !git clone https://github.com/1kaiser/BoostingMonocularDepth.git\n",
        "\n",
        "# # Downloading merge model weights \n",
        "# !mkdir -p /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "# !gdown 1BbX2_NjrmrzRIB2CRJ-1rNRAjPABXPCh\n",
        "# !mv latest_net_G.pth /content/BoostingMonocularDepth/pix2pix/checkpoints/mergemodel/\n",
        "\n",
        "# # Downloading Midas weights\n",
        "# #!gdown https://drive.google.com/uc?id=1nqW_Hwj86kslfsXR7EnXpEWdO2csz1cC\n",
        "# #!mv model.pt /content/BoostingMonocularDepth/midas/\n",
        "\n",
        "# # # Downloading LeRes weights \n",
        "# !gdown 1vni3Oh4y0S4eatz_u_aDnwDLwxBjUpCh\n",
        "# !mv res101.pth /content/BoostingMonocularDepth/"
      ],
      "metadata": {
        "id": "MHjb_2jxbHjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code section 3\n",
        "%cd BoostingMonocularDepth/"
      ],
      "metadata": {
        "id": "rAOyd3jnbfRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check and delete  fiiles"
      ],
      "metadata": {
        "id": "QCNFJkXaR-ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FILES = '/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive/input1'\n",
        "OUTPUT_FILES = \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive/outputs_leres\""
      ],
      "metadata": {
        "id": "aF3FVhNbSPNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile goDeleteHolistic.js\n",
        "'use strict';\n",
        "\n",
        "const fs = require('fs');\n",
        "let d=0;\n",
        "let darray1 = fs.readdirSync({INPUT_FILES});\n",
        "let darray2 = fs.readdirSync({OUTPUT_FILES});\n",
        "for (let index = 0; index < darray1.length; index++) {\n",
        "    console.log(index)\n",
        "    if (darray2.includes(darray1[index])) {\n",
        "        console.log(d++)\n",
        "        \n",
        "        fs.unlinkSync({INPUT_FILES}+darray1[index])\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "xoDYwaPER849"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!node goDeleteHolistic.js"
      ],
      "metadata": {
        "id": "tFtDRDGcSDMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the method using LeRes\n",
        "!python run.py --Final --data_dir {INPUT_FILES} --output_dir  {OUTPUT_FILES}/ --depthNet 2"
      ],
      "metadata": {
        "id": "gDgQ4X_ubiNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Video Output**"
      ],
      "metadata": {
        "id": "ZIBs_0aDbq46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create video 1\n",
        "!ffmpeg \\\n",
        "-framerate 24 \\\n",
        "-pattern_type glob \\\n",
        "-i {INPUT_FILES}'/*.png' \\\n",
        "input0.mp4\n",
        "#create vodeo 2\n",
        "!ffmpeg \\\n",
        "-framerate 24 \\\n",
        "-pattern_type glob \\\n",
        "-i {OUTPUT_FILES}'/*.png' \\\n",
        "input1.mp4\n",
        "#merge video 1 and video 2\n",
        "!ffmpeg \\\n",
        "-i input0.mp4 -i input1.mp4 \\\n",
        "-filter_complex \\\n",
        "\"[0:v][1:v]vstack=inputs=2\" \\\n",
        "finalOutput.mp4\n",
        "#add sound of main video to final output video\n",
        "!ffmpeg \\\n",
        "-i 'finalOutput.mp4' -i 'm.mp4' \\\n",
        "-c:v copy -c:a copy -map 0:v:0 -map 1:a:0 \\\n",
        "OUTPUT_FILE.mp4"
      ],
      "metadata": {
        "id": "IHv3_wz1bs4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzkIeHiNc9Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pd0LfKcGizVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/OUT/data/machine_learning_test_dataset/drive"
      ],
      "metadata": {
        "id": "rPX-Npz7qQYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "EF4kDZevqkmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FFMPEG video to image**"
      ],
      "metadata": {
        "id": "BpmpJgkHa0fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**MLP MODEL WITH POSITIONAL ENCODER**"
      ],
      "metadata": {
        "id": "UFAs21IJafFS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icmWdrOM9Rx1"
      },
      "outputs": [],
      "source": [
        "############<< LOADING DATASET >>#############\n",
        "\n",
        "!cp \"/content/drive/MyDrive/OUT/data/machine_learning_test_dataset/archive.zip\" \"/content/\"\n",
        "!unzip /content/archive.zip #unzipping the flower images from archive..\n",
        "!mkdir -p /content/flowers/all\n",
        "!cp /content/flowers/daisy/* /content/flowers/all\n",
        "!cp /content/flowers/dandelion/* /content/flowers/all\n",
        "!cp /content/flowers/rose/* /content/flowers/all\n",
        "!cp /content/flowers/sunflower/* /content/flowers/all\n",
        "!cp /content/flowers/tulip/* /content/flowers/all\n",
        "\n",
        "##############$$$$$$$$$$$$$$<<< IMPORTING LIBRARIES >>>$$$$$$$$$$$$$$$$################\n",
        "#✅\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "!python -m pip install -q -U flax\n",
        "import flax\n",
        "import optax\n",
        "from typing import Any\n",
        "\n",
        "from jax import lax\n",
        "import flax.linen as nn\n",
        "from flax.training import train_state\n",
        "##########################################<< FFT PSEUDO SIN COS POSITIONAL ENCODER >>#########################################\n",
        "\n",
        "positional_encoding_dims = 6  # Number of positional encodings applied\n",
        "\n",
        "def positional_encoding(inputs):\n",
        "    print(\"positional_encoding start\")\n",
        "    batch_size, _ = inputs.shape;print(inputs.shape)\n",
        "    inputs_freq = jax.vmap(lambda x: inputs * 2.0 ** x)(jnp.arange(positional_encoding_dims));print(inputs_freq.shape)\n",
        "    x = jnp.stack([jnp.sin(inputs_freq), jnp.cos(inputs_freq)]);print(x.shape)\n",
        "    x = x.swapaxes(0, 2);print(x.shape)\n",
        "    x = x.reshape([batch_size, -1]);print(x.shape)\n",
        "    x = jnp.concatenate([inputs, x], axis=-1);print(x.shape)\n",
        "    print(\"positional_encoding end\")\n",
        "    return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "\n",
        "apply_positional_encoding = True # Apply posittional encoding to the input or not\n",
        "num_dense_layers = 8 # Number of dense layers in MLP\n",
        "dense_layer_width = 256 # Dimentionality of dense layers' output space \n",
        "\n",
        "class MLPModel(nn.Module):\n",
        "    dtype: Any = jnp.float32\n",
        "    precision: Any = lax.Precision.DEFAULT\n",
        "    apply_positional_encoding: bool = apply_positional_encoding\n",
        "    @nn.compact\n",
        "    def __call__(self, input_points):\n",
        "        x = positional_encoding(input_points) if self.apply_positional_encoding else input_points\n",
        "        print(\"network model start\")\n",
        "        print(x.shape)\n",
        "        for i in range(num_dense_layers):\n",
        "            x = nn.Dense(\n",
        "                dense_layer_width,\n",
        "                dtype=self.dtype,\n",
        "                precision=self.precision\n",
        "                )(x)\n",
        "            x = nn.relu(x)\n",
        "            x = jnp.concatenate([x, input_points], axis=-1) if i == 4 else x\n",
        "            print(x.shape)\n",
        "  \n",
        "        x = nn.Dense(1, dtype=self.dtype, precision=self.precision)(x)\n",
        "        print(x.shape)\n",
        "        print(\"network model end\")\n",
        "        return x\n",
        "##########################################<< MLP MODEL >>#########################################\n",
        "     \n",
        "##########################################<< CREATE MODEL INITIAL STATE FILLING WITH WEIGHTS >>#########################################\n",
        "\n",
        "def init_train_state(model, r_key, shape, learning_rate ) -> train_state.TrainState:\n",
        "    print(shape)\n",
        "    init_variables = model.init(r_key, jnp.ones(shape))  # Initialize the Model\n",
        "    optimizer = optax.adam(learning_rate) # Create the optimizer\n",
        "    # Create a State\n",
        "    return train_state.TrainState.create(\n",
        "        apply_fn = model.apply,\n",
        "        tx=optimizer,\n",
        "        params=init_variables['params']\n",
        "    )\n",
        "\n",
        "learning_rate = 1e-4\n",
        "batch_size_no = 64\n",
        "\n",
        "model = MLPModel() # Instantiate the Model\n",
        "\n",
        "key, rng = jax.random.split(jax.random.PRNGKey(0))\n",
        "x = jnp.empty((batch_size_no, 28, 28, 1)) # Dummy Input\n",
        "_, image_height, image_width, channels = x.shape\n",
        "##########################################<< CREATING MODEL STATE>>#########################################\n",
        "\n",
        "state = init_train_state( model, rng, (image_height * image_width, channels), learning_rate )\n",
        "state\n",
        "print(\"state initiated ✅\")\n",
        "\n",
        "##########################################<< MODEL LOSS METRICS FOR GRADIENT COMPUTATION >>#########################################\n",
        "def image_difference_loss(logits, labels):\n",
        "    loss = .5 * jnp.mean((logits - labels) ** 2)\n",
        "    return loss\n",
        "def compute_metrics(*, logits, labels):\n",
        "  loss = image_difference_loss(logits, labels)\n",
        "  loss = lax.pmean(loss, axis_name=\"batch\");print(\"ok4\")\n",
        "  metrics = {\n",
        "      'loss': loss,\n",
        "      'logits': logits, #PREDICTED IMAGE\n",
        "      'labels': labels  #ACTUAL IMAGE\n",
        "  }\n",
        "  return metrics\n",
        "\n",
        "##########################################<< MODEL TRAINING STEP FOR UPDATING WEIGHTS FROM CALCULATED GRAGIENTS FROM CHANGING MODEL STATE >>#########################################\n",
        "import jax\n",
        "\n",
        "def train_step(state: train_state.TrainState, batch: jnp.asarray, rng):\n",
        "    print(batch)\n",
        "    image, label = batch\n",
        "    print(image,\"<<<image\")\n",
        "    print(label,\"<<<label\")    \n",
        "    def loss_fn(params):\n",
        "        logits = state.apply_fn({'params': params}, image);print(\"done1\",logits.shape)\n",
        "        loss =  image_difference_loss(logits, label);print(\"done2\",loss.shape)\n",
        "        return loss, logits\n",
        "\n",
        "    print(\"ok1really\")\n",
        "    gradient_fn = jax.value_and_grad(loss_fn, has_aux=True);print(\"ok1\")\n",
        "    (_, logits), grads = gradient_fn(state.params);print(\"ok2\")\n",
        "    #train_loss, gradients_each = jax.value_and_grad(loss_fn)(state.params);print(\"ok3\")\n",
        "    grads = lax.pmean(grads,\"batch\");print(\"ok4\")\n",
        "    # grads = jnp.mean(grads);print(\"ok4\")\n",
        "    state = state.apply_gradients(grads=grads);print(\"ok5\")\n",
        "    # train_loss = jnp.mean(train_loss);print(\"ok6\")\n",
        "    logs = compute_metrics(logits=logits, labels=label);print(\"ok7\")\n",
        "    return state, logs\n",
        "\n",
        "parallel_train_step = jax.pmap(train_step, \"batch\")\n",
        "# parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", in_axes = (0, 0, 0))\n",
        "\n",
        "import jax\n",
        "@jax.jit\n",
        "def eval_step(state, batch):\n",
        "    image, label = batch\n",
        "    logits = state.apply_fn({'params': state.params}, image)\n",
        "    return compute_metrics(logits=logits, labels=label)\n",
        "\n",
        "##########################################<< DATASET GENERATING FUNCTION HELPER IN GRB TO GRAY CONVERSION >>#########################################\n",
        "newsize = (150, 150) # /.... 233 * 454\n",
        "from PIL import Image\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def imageGRAY(argv):\n",
        "    im = Image.open(argv).convert('L')\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,1)\n",
        "    return tvt, tvu\n",
        "def imageRGB(argv):\n",
        "    im = Image.open(argv)\n",
        "    tvt, tvu = jnp.asarray(im.resize(newsize)),jnp.asarray(im.resize(newsize)).reshape(-1,3)\n",
        "    return tvt, tvu\n",
        "\n",
        "\n",
        "##########################################<< ANALYSING DATASETS FOR BATCH DISTRIBUTION >>#########################################\n",
        "# testing batching of the dataset frfom the total dataset, by assuming batch size as 8 then running 50 epochs over the batch , then moving processing onto next batch>>>\n",
        "batch_size = jax.device_count()    #///// batchsize depending on number of devices available for processing\n",
        "import os\n",
        "image_dir = r'/content/flowers/all/'\n",
        "#############################################################################\n",
        "prefix = \"sur_refl_\"\n",
        "bandend = [\"c\",\".jpg\", \"b02\", \"b03\", \"b04\", \"b05\", \"b06\", \"b07\", \"day_of_year\", \"qc_500m\", \"raz\", \"state_500m\", \"szen\", \"vzen\"]\n",
        "DayOY = \"_doy\\[0-9]+_aid0001\"\n",
        "fileExt = r'.jpg'\n",
        "expression_b2 = bandend[1]\n",
        "total_images =  [f for f in os.listdir(image_dir) if f.__contains__(expression_b2)]\n",
        "total_images.sort()\n",
        "total_images_path = [os.path.join(image_dir, i) for i in total_images if i != 'outputs']\n",
        "no_of_batches = int(len(total_images_path)/batch_size)\n",
        "\n",
        "\n",
        "\n",
        "##########################################<< RUNNING TRAINNING >>#########################################\n",
        "#@title # **👠HIGH HEELS RUN >>>>>>>>>>>** { vertical-output: true }\n",
        "newsize = (260, 260) # /.... 233 * 454\n",
        "\n",
        "import jax\n",
        "from jax import random\n",
        "def batchedimages(image_locations):\n",
        "  ddyss = jnp.asarray((imageRGB(total_images_path[image_locations[0]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[1]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[2]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[3]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[4]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[5]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[6]])[1],\n",
        "                      imageRGB(total_images_path[image_locations[7]])[1]))\n",
        "  ddxss = jnp.asarray((imageGRAY(total_images_path[image_locations[0]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[1]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[2]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[3]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[4]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[5]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[6]])[1],\n",
        "                      imageGRAY(total_images_path[image_locations[7]])[1]))\n",
        "  #print(ddyss.shape,\"<<<< ddyss.shape???\",ddxss.shape,\"<<<< ddxss.shape???\") #to check shape \n",
        "  batch_ccc = ddyss, ddxss\n",
        "  return batch_ccc\n",
        "  \n",
        "def data_stream():\n",
        "  key = random.PRNGKey(0)\n",
        "  perm = random.permutation(key, len(total_images_path))\n",
        "  for i in range(no_of_batches):\n",
        "    batch_idx = perm[i * batch_size : (i + 1) * batch_size]; #print(batch_idx)\n",
        "    yield batchedimages(batch_idx)\n",
        "\n",
        "batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "\n",
        "next(batches)[0].shape ### this command starts initial 8 image  stream, if callled inside a iteration loop then it will get next images for calculations>>>\n",
        "vv, shapea, channels = next(batches)[0].shape # seitting values >>> [0] 8 784 3  [1] 8 784 1 ; RGB & GRAYSCALE versions 8 images each converted to 1-D array\n",
        "print(vv, shapea, channels)\n",
        "######################<<< summary writer for tensor board\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# logdir = \"runs\"\n",
        "# writer = SummaryWriter(logdir)\n",
        "######################\n",
        "######################\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "#################################<<< checking if checkpoint already available\n",
        "import os # importing os module\n",
        "import re # to find file using regular expression\n",
        "checkpoint_available = 0\n",
        "pattern = re.compile(\"checkpoint_\\d+\")   # to search for \"checkpoint_*munerical value*\" numerical value of any length is denoted by regular expression \"\\d+\"\n",
        "dir = \"/content/ckpts/\"\n",
        "isFile = os.path.isdir(dir)\n",
        "if isFile:\n",
        "  for filepath in os.listdir(dir):\n",
        "      if pattern.match(filepath):\n",
        "          checkpoint_available = 1\n",
        "#################################\n",
        "##########################################<<< loading checkpoint by checking the Flag available\n",
        "from flax.training import checkpoints\n",
        "if checkpoint_available:\n",
        "  CKPT_DIR = 'ckpts'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "  #state = flax.jax_utils.replicate(restored_state)\n",
        "  print(\"true <<< File loaded for and replicated to all devices\")\n",
        "##########################################\n",
        "######################<<<< initiating train state\n",
        "count = 0\n",
        "if count == 0 :\n",
        "  state = init_train_state( model, rng, (shapea, channels), learning_rate ) \n",
        "  count = 1\n",
        "state = flax.jax_utils.replicate(state)  # FLAX will replicate the state to every device so that updating can be made easy\n",
        "dropout_rngs = jax.random.split(rng, jax.local_device_count())\n",
        "######################\n",
        "from flax.training import checkpoints\n",
        "import time\n",
        "total_epochs = 50\n",
        "for epochs in range(total_epochs):   # EPOCHS for training & updating the initiated state, metrics may show the loss in each epochs or iteration\n",
        "  start_time = time.time()\n",
        "  batches = data_stream()  ### this stream will utilize the array of paths of images to a folder, then \"generate\" batches into the variable\n",
        "  if checkpoint_available:\n",
        "    CKPT_DIR = 'ckpts'\n",
        "    restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state)\n",
        "    state = restored_state\n",
        "    checkpoint_available = 0 # << Flag updated >>> to stop loading the same checkpoint in the next iteration then remove the checkpoint directory\n",
        "    !rm -r /content/ckpts\n",
        "  for bbb in range(no_of_batches-5):\n",
        "    print(bbb,\"of total number of batches\",no_of_batches)\n",
        "    state, metrics = parallel_train_step(state, next(batches), dropout_rngs)\n",
        "    print(\"<<✅✅✅epoc : \",epochs,\" complete✅✅✅>>\\n\",metrics['loss'][0]) #printing loss 1 out of 8 processed in 8 devices\n",
        "    \n",
        "    #############################################<<< output visualization 👍🏻\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    import numpy as np        # source : https://www.folkstalk.com/tech/how-to-convert-numpy-array-to-cv2-image-with-code-examples/\n",
        "    \n",
        "    print(\"logits shape ⚡⚡⚡⚡\", metrics['logits'][0].shape)\n",
        "    L1 = metrics['logits'][0]\n",
        "    # predicted_image = np.array(L1,  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "    # cv2img = cv2_imshow(predicted_image) # This work the same as passing an image\n",
        "    for i in range(0,metrics['labels'].shape[0]):\n",
        "      print(i)\n",
        "      predicted_image = np.array(metrics['logits'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array\n",
        "      cv2_imshow(predicted_image)\n",
        "\n",
        "      print(\"labels shape ⚡⚡⚡⚡\", metrics['labels'][0].shape)\n",
        "      L2 = metrics['labels'][0]\n",
        "      actual_image = np.array(metrics['labels'][i],  dtype=np.uint8).reshape(newsize) # This would be your image array newsize = image_width , image_height\n",
        "      cv2img = cv2_imshow(actual_image) # This work the same as passing an image\n",
        "    #############################################<<< output visualization 👍🏻\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    writer.add_scalar('Loss', int(metrics['loss'][0]), epochs)\n",
        "  epoch_time = time.time() - start_time\n",
        "  print(f\"Epoch {epochs} in {epoch_time:0.2f} sec\")\n",
        "  ##################################################<<< model saving mechanism for flax model state as checkpoints for each epochs,\"checkpoint\" is a terminology that means all the model weights and biases during the calculation till the completion of 1 epoch were being updated, then this final set of weights and biases including their placement inside the model will be saves as a (schema+weight values) saved as checkpoint in the mentioned <<CKPT_DIR = 'ckpts'>> mentioned folder.  \n",
        "  CKPT_DIR = 'ckpts'\n",
        "  checkpoints.save_checkpoint(ckpt_dir=CKPT_DIR, target=state, step= epochs)     # naming of the checkpoint is \"checkpoint_*\"  where \"*\" => value of the steps variable, i.e. 'epochs'\n",
        "  restored_state = checkpoints.restore_checkpoint(ckpt_dir=CKPT_DIR, target=state) # using to get the checkpoint loaded , it can be latest one , or if already available as checkpoint in the \"CKPT_DIR\" directory then take the file from directory then save in >> restored_checkpoints\n",
        "  ##################################################\n",
        "  # images = total_images_path[batch_idx]\n",
        "writer.flush()\n",
        "\n",
        " "
      ]
    }
  ]
}